---
tags:
  - data
link: https://habr.com/ru/companies/ods/articles/328372/
source: habr
data_type:
  - DS
company: ODS
author: ODS
---
Привет, Хабр!

  
![](https://habrastorage.org/r/w1560/web/b33/683/d49/b33683d495964ebb9ca8d413e60c4085.jpg)  

В задачах машинного обучения для оценки качества моделей и сравнения различных алгоритмов используются метрики, а их выбор и анализ — непременная часть работы датасатаниста.

В этой статье мы рассмотрим некоторые критерии качества в задачах классификации, обсудим, что является важным при выборе метрики и что может пойти не так.


# Метрики в задачах классификации

  
Для демонстрации полезных функций _sklearn_ и наглядного представления метрик мы будем использовать [датасет](https://bigml.com/user/francisco/gallery/dataset/5163ad540c0b5e5b22000383) по оттоку клиентов телеком-оператора.

**Загрузим необходимые библиотеки и посмотрим на данные**
 

```
df.head(5)
```

  
![](https://habrastorage.org/r/w1560/web/cc3/55e/98b/cc355e98bdac4b8a9e0a12261b5f849b.png)  

**Предобработка данных**

  

## Accuracy, precision и recall

  

Перед переходом к самим метрикам необходимо ввести важную концепцию для описания этих метрик в терминах ошибок классификации — _confusion matrix_ (матрица ошибок).  
Допустим, что у нас есть два класса и алгоритм, предсказывающий принадлежность каждого объекта одному из классов, тогда матрица ошибок классификации будет выглядеть следующим образом:

  

||![$y = 1$](https://habrastorage.org/getpro/habr/post_images/447/bea/1c7/447bea1c7af9dca25d20da8fede95184.svg)|![$y = 0$](https://habrastorage.org/getpro/habr/post_images/12a/4a8/632/12a4a86327cbde9aa9fec271ce37a205.svg)|
|---|---|---|
|![$\hat y = 1$](https://habrastorage.org/getpro/habr/post_images/818/8c3/192/8188c3192189c41564bd2ae4a03a9d91.svg)|True Positive (TP)|False Positive (FP)|
|![$\hat y = 0$](https://habrastorage.org/getpro/habr/post_images/74d/5cb/ba3/74d5cbba34162f133a65cbbe32164dd7.svg)|False Negative (FN)|True Negative (TN)|

  

Здесь ![$\hat y$](https://habrastorage.org/getpro/habr/post_images/8b1/e99/65f/8b1e9965f298b958fcd6ae9f77c60704.svg) — это ответ алгоритма на объекте, а ![$y$](https://habrastorage.org/getpro/habr/post_images/1e6/829/a25/1e6829a256c8a2712f849dc1d2343b62.svg) — истинная метка класса на этом объекте.  
Таким образом, ошибки классификации бывают двух видов: False Negative (FN) и False Positive (FP).

  

**Обучение алгоритма и построение матрицы ошибок**

  

![](https://habrastorage.org/r/w1560/web/37d/d24/a92/37dd24a92c8e4fe1aaebe31618930473.png)

  

#### Accuracy

  

Интуитивно понятной, очевидной и почти неиспользуемой метрикой является accuracy — доля правильных ответов алгоритма:

  

![$\large accuracy = \frac{TP + TN}{TP + TN + FP + FN}$](https://habrastorage.org/getpro/habr/post_images/3a6/cb8/2da/3a6cb82da231408bd4c38a70e59fb1d8.svg)

  

Эта метрика бесполезна в задачах с неравными классами, и это легко показать на примере.

  

Допустим, мы хотим оценить работу спам-фильтра почты. У нас есть 100 не-спам писем, 90 из которых наш классификатор определил верно (True Negative = 90, False Positive = 10), и 10 спам-писем, 5 из которых классификатор также определил верно (True Positive = 5, False Negative = 5).  
Тогда accuracy:

  

![$\ accuracy = \frac{5 + 90}{5 + 90 + 10 + 5} = 86,4% $](https://habrastorage.org/getpro/habr/post_images/35f/ff7/01d/35fff701dae36207f6bf3d536f366fc1.svg)

  

Однако если мы просто будем предсказывать все письма как не-спам, то получим более высокую accuracy:

  

![$\ accuracy = \frac{0 + 100}{0 + 100 + 0 + 10} = 90,9% $](https://habrastorage.org/getpro/habr/post_images/6fc/81d/a6e/6fc81da6e0a9354fca9491b22b3981f4.svg)

  

При этом, наша модель совершенно не обладает никакой предсказательной силой, так как изначально мы хотели определять письма со спамом. Преодолеть это нам поможет переход с общей для всех классов метрики к отдельным показателям качества классов.

  

#### Precision, recall и F-мера

  

Для оценки качества работы алгоритма на каждом из классов по отдельности введем метрики precision (точность) и recall (полнота).

  

![$\large precision = \frac{TP}{TP + FP}$](https://habrastorage.org/getpro/habr/post_images/164/93b/c89/16493bc899f7275f3b5ff8d45a3ed2e2.svg)

  

![$\large recall = \frac{TP}{TP + FN}$](https://habrastorage.org/getpro/habr/post_images/258/4e7/8f3/2584e78f32225eade5cb8b1b4a665193.svg)

  

Precision можно интерпретировать как долю объектов, названных классификатором положительными и при этом действительно являющимися положительными, а recall показывает, какую долю объектов положительного класса из всех объектов положительного класса нашел алгоритм.

  

![](https://habrastorage.org/r/w1560/web/38e/9d4/892/38e9d4892d9241ea95e1f56e3ef9124c.png)

  

Именно введение precision не позволяет нам записывать все объекты в один класс, так как в этом случае мы получаем рост уровня False Positive. Recall демонстрирует способность алгоритма обнаруживать данный класс вообще, а precision — способность отличать этот класс от других классов.

  

Как мы отмечали ранее, ошибки классификации бывают двух видов: False Positive и False Negative. В статистике первый вид ошибок называют ошибкой I-го рода, а второй — ошибкой II-го рода. В нашей задаче по определению оттока абонентов, ошибкой первого рода будет принятие лояльного абонента за уходящего, так как наша [нулевая гипотеза](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors) состоит в том, что никто из абонентов не уходит, а мы эту гипотезу отвергаем. Соответственно, ошибкой второго рода будет являться "пропуск" уходящего абонента и ошибочное принятие нулевой гипотезы.

  

Precision и recall не зависят, в отличие от accuracy, от соотношения классов и потому применимы в условиях несбалансированных выборок.  
Часто в реальной практике стоит задача найти оптимальный (для заказчика) баланс между этими двумя метриками. Классическим примером является задача определения оттока клиентов.  
Очевидно, что мы не можем находить **всех** уходящих в отток клиентов и **только** их. Но, определив стратегию и ресурс для удержания клиентов, мы можем подобрать нужные пороги по precision и recall. Например, можно сосредоточиться на удержании только высокодоходных клиентов или тех, кто уйдет с большей вероятностью, так как мы ограничены в ресурсах колл-центра.

  

Обычно при оптимизации гиперпараметров алгоритма (например, в случае перебора по сетке [_GridSearchCV_](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) ) используется одна метрика, улучшение которой мы и ожидаем увидеть на тестовой выборке.  
Существует несколько различных способов объединить precision и recall в агрегированный критерий качества. F-мера (в общем случае ![$\ F_\beta$](https://habrastorage.org/getpro/habr/post_images/7c3/bf7/0f2/7c3bf70f29a2fa3bc5f72bd3dcf3a579.svg)) — среднее гармоническое precision и recall :

  

![$\large \ F_\beta = (1 + \beta^2) \cdot \frac{precision \cdot recall}{(\beta^2 \cdot precision) + recall}$](https://habrastorage.org/getpro/habr/post_images/4f1/f5c/0ab/4f1f5c0ab5dcf8379ff4bcbbd02f0623.svg)

  

![$\beta$](https://habrastorage.org/getpro/habr/post_images/f39/05b/5cf/f3905b5cfab08d98b2d380d5ea75c66c.svg) в данном случае определяет вес точности в метрике, и при ![$\beta = 1$](https://habrastorage.org/getpro/habr/post_images/a66/57a/5ad/a6657a5ad779fd051b6a6b0fe3464c51.svg) это среднее гармоническое (с множителем 2, чтобы в случае precision = 1 и recall = 1 иметь ![$\ F_1 = 1$](https://habrastorage.org/getpro/habr/post_images/8eb/d7a/a05/8ebd7aa050e5cab1d1d63f1eb7f1e366.svg))  
F-мера достигает максимума при полноте и точности, равными единице, и близка к нулю, если один из аргументов близок к нулю.  
В sklearn есть удобная функция _metrics.classification_report_, возвращающая recall, precision и F-меру для каждого из классов, а также количество экземпляров каждого класса.

  

```
report = classification_report(y_test, lr.predict(X_test), target_names=['Non-churned', 'Churned'])
print(report)
```

  

|class|precision|recall|f1-score|support|
|---|---|---|---|---|
|Non-churned|0.88|0.97|0.93|941|
|Churned|0.60|0.25|0.35|159|
|avg / total|0.84|0.87|0.84|1100|

  

Здесь необходимо отметить, что в случае задач с несбалансированными классами, которые превалируют в реальной практике, часто приходится прибегать к техникам искусственной модификации датасета для выравнивания соотношения классов. Их существует много, и мы не будем их касаться, [здесь](http://contrib.scikit-learn.org/imbalanced-learn/) можно посмотреть некоторые методы и выбрать подходящий для вашей задачи.

  

#### AUC-ROC и AUC-PR

  

При конвертации вещественного ответа алгоритма (как правило, вероятности принадлежности к классу, отдельно см. [SVM](http://scikit-learn.org/stable/modules/svm.html#scores-probabilities)) в бинарную метку, мы должны выбрать какой-либо порог, при котором 0 становится 1. Естественным и близким кажется порог, равный 0.5, но он не всегда оказывается оптимальным, например, при вышеупомянутом отсутствии баланса классов.

  

Одним из способов оценить модель в целом, не привязываясь к конкретному порогу, является AUC-ROC (или ROC AUC) — площадь (_A_rea _U_nder _C_urve) под кривой ошибок (_R_eceiver _O_perating _C_haracteristic curve ). Данная кривая представляет из себя линию от (0,0) до (1,1) в координатах True Positive Rate (TPR) и False Positive Rate (FPR):

  

![$\large TPR = \frac{TP}{TP + FN}$](https://habrastorage.org/getpro/habr/post_images/71e/20f/f85/71e20ff859277c93efc4a1cb20fd4f7a.svg)

  

![$\large FPR = \frac{FP}{FP + TN}$](https://habrastorage.org/getpro/habr/post_images/f75/d27/d27/f75d27d279b274eedc5004a691ccb18a.svg)

  

TPR нам уже известна, это полнота, а FPR показывает, какую долю из объектов negative класса алгоритм предсказал неверно. В идеальном случае, когда классификатор не делает ошибок (FPR = 0, TPR = 1) мы получим площадь под кривой, равную единице; в противном случае, когда классификатор случайно выдает вероятности классов, AUC-ROC будет стремиться к 0.5, так как классификатор будет выдавать одинаковое количество TP и FP.  
Каждая точка на графике соответствует выбору некоторого порога. Площадь под кривой в данном случае показывает качество алгоритма (больше — лучше), кроме этого, важной является крутизна самой кривой — мы хотим максимизировать TPR, минимизируя FPR, а значит, наша кривая в идеале должна стремиться к точке (0,1).

  

**Код отрисовки ROC-кривой**

  

![](https://habrastorage.org/r/w1560/web/299/157/fad/299157fad56a4ecca8f6b96b425bd38c.png)

  

Критерий AUC-ROC устойчив к несбалансированным классам (спойлер: увы, не всё так однозначно) и может быть интерпретирован как вероятность того, что случайно выбранный positive объект будет проранжирован классификатором выше (будет иметь более высокую вероятность быть positive), чем случайно выбранный negative объект.

  

Рассмотрим следующую задачу: нам необходимо выбрать 100 релевантных документов из 1 миллиона документов. Мы намашинлернили два алгоритма:

  

- **Алгоритм 1** возвращает 100 документов, 90 из которых релевантны. Таким образом,

  

![$ TPR = \frac{TP}{TP + FN} = \frac{90}{90 + 10} = 0.9$](https://habrastorage.org/getpro/habr/post_images/025/614/701/025614701b5a3780f0284941f27807af.svg)

  

![$ FPR = \frac{FP}{FP + TN} = \frac{10}{10 + 999890} = 0.00001$](https://habrastorage.org/getpro/habr/post_images/069/ba6/64f/069ba664fd6cf79f32077d8d97e82822.svg)

  

- **Алгоритм 2** возвращает 2000 документов, 90 из которых релевантны. Таким образом,

  

![$ TPR = \frac{TP}{TP + FN} = \frac{90}{90 + 10} = 0.9$](https://habrastorage.org/getpro/habr/post_images/025/614/701/025614701b5a3780f0284941f27807af.svg)

  

![$ FPR = \frac{FP}{FP + TN} = \frac{1910}{1910 + 997990} = 0.00191$](https://habrastorage.org/getpro/habr/post_images/8b9/2d6/8c6/8b92d68c64dd425e82a61736a193bad3.svg)

  

Скорее всего, мы бы выбрали первый алгоритм, который выдает очень мало False Positive на фоне своего конкурента. Но разница в False Positive Rate между этими двумя алгоритмами _крайне_ мала — всего 0.0019. Это является следствием того, что AUC-ROC измеряет долю False Positive относительно True Negative и в задачах, где нам не так важен второй (больший) класс, может давать не совсем адекватную картину при сравнении алгоритмов.

  

Для того чтобы поправить положение, вернемся к полноте и точности :

  

- **Алгоритм 1**

  

![$\ precision = \frac{TP}{TP + FP} = 90/(90 + 10) = 0.9 $](https://habrastorage.org/getpro/habr/post_images/f0f/ee9/2db/f0fee92db0bbd42834e684013d6c7adb.svg)

  

![$\ recall = \frac{TP}{TP + FN} = 90/(90 + 10) = 0.9 $](https://habrastorage.org/getpro/habr/post_images/525/b48/226/525b482263b78da07ec0413edd70d16b.svg)

  

- **Алгоритм 2**

  

![$\ precision = \frac{TP}{TP + FP} = \frac{90}{90 + 1910} = 0.045 $](https://habrastorage.org/getpro/habr/post_images/d91/2da/f29/d912daf29a8d59b4641930add0b284d3.svg)

  

![$\ recall = \frac{TP}{TP + FN} = \frac{90}{90 + 10} = 0.9 $](https://habrastorage.org/getpro/habr/post_images/169/a14/cf3/169a14cf303ee39cfc422a779e37b55d.svg)

  

Здесь уже заметна существенная разница между двумя алгоритмами — 0.855 в точности!

  

Precision и recall также используют для построения кривой и, аналогично AUC-ROC, находят площадь под ней.

  

![](https://habrastorage.org/r/w1560/web/887/356/914/8873569147c24b49a2b7fa9ae883f20f.png)

  

Здесь можно отметить, что на маленьких датасетах площадь под PR-кривой может быть чересчур оптимистична, потому как вычисляется по методу трапеций, но обычно в таких задачах данных достаточно. За подробностями о взаимоотношениях AUC-ROC и AUC-PR можно обратиться [сюда](http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf).

  

#### Logistic Loss

  

Особняком стоит логистическая функция потерь, определяемая как:

  

![$\large logloss = - \frac{1}{l} \cdot \sum_{i=1}^l (y_i \cdot log(\hat y_i) + (1 - y_i) \cdot log(1 - \hat y_i))$](https://habrastorage.org/getpro/habr/post_images/c54/acb/f01/c54acbf01a8ad65e36a632819be32a2b.svg)

  

здесь ![$\hat y$](https://habrastorage.org/getpro/habr/post_images/8b1/e99/65f/8b1e9965f298b958fcd6ae9f77c60704.svg) — это ответ алгоритма на ![$i$](https://habrastorage.org/getpro/habr/post_images/721/95b/0db/72195b0dbc8d5e1fbd3b06fd91ae2dba.svg)-ом объекте, ![$y$](https://habrastorage.org/getpro/habr/post_images/1e6/829/a25/1e6829a256c8a2712f849dc1d2343b62.svg) — истинная метка класса на ![$i$](https://habrastorage.org/getpro/habr/post_images/721/95b/0db/72195b0dbc8d5e1fbd3b06fd91ae2dba.svg)-ом объекте, а ![$l$](https://habrastorage.org/getpro/habr/post_images/ce3/8a8/387/ce38a83878bf16954ea60627fc7c66eb.svg) размер выборки.

  

Подробно про математическую интерпретацию логистической функции потерь уже написано в рамках [поста](https://habrahabr.ru/company/ods/blog/323890/#princip-maksimalnogo-pravdopodobiya-i-logisticheskaya-regressiya) про линейные модели.  
Данная метрика нечасто выступает в бизнес-требованиях, но часто — в задачах на [kaggle](http://kaggle.com/).  
Интуитивно можно представить минимизацию logloss как задачу максимизации accuracy путем штрафа за неверные предсказания. Однако необходимо отметить, что logloss крайне сильно штрафует за уверенность классификатора в неверном ответе.

  

Рассмотрим пример:

  

```
def logloss_crutch(y_true, y_pred, eps=1e-15):

    return - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

print('Logloss при неуверенной классификации %f' % logloss_crutch(1, 0.5))
>> Logloss при неуверенной классификации 0.693147

print('Logloss при уверенной классификации и верном ответе %f' % logloss_crutch(1, 0.9))
>> Logloss при уверенной классификации и верном ответе 0.105361

print('Logloss при уверенной классификации и НЕверном ответе %f' % logloss_crutch(1, 0.1))
>> Logloss при уверенной классификации и НЕверном ответе 2.302585
```

  

Отметим, как драматически выросла logloss при неверном ответе и уверенной классификации!  
Следовательно, ошибка на одном объекте может дать существенное ухудшение общей ошибки на выборке. Такие объекты часто бывают выбросами, которые нужно не забывать фильтровать или рассматривать отдельно.  
Всё становится на свои места, если нарисовать график logloss:

  

![](https://habrastorage.org/r/w1560/web/0fb/e4e/da3/0fbe4eda3cb3469c8d02c837b53e564e.png)

  

Видно, что чем ближе к нулю ответ алгоритма при ground truth = 1, тем выше значение ошибки и круче растёт кривая.

  

## Подытожим:

  

- В случае многоклассовой классификации нужно внимательно следить за метриками каждого из классов и следовать логике решения **задачи**, а не оптимизации метрики
- В случае неравных классов нужно подбирать баланс классов для обучения и метрику, которая будет корректно отражать качество классификации
- Выбор метрики нужно делать с фокусом на предметную область, предварительно обрабатывая данные и, возможно, сегментируя (как в случае с делением на богатых и бедных клиентов)

  

## Полезные ссылки

  

1. Курс Евгения Соколова: [Семинар по выбору моделей](https://github.com/esokolov/ml-course-msu/blob/master/ML15/lecture-notes/Sem05_metrics.pdf) (там есть информация по метрикам задач регрессии)
2. [Задачки](https://alexanderdyakonov.wordpress.com/2015/10/09/%D0%B7%D0%B0%D0%B4%D0%B0%D1%87%D0%BA%D0%B8-%D0%BF%D1%80%D0%BE-auc-roc/) на AUC-ROC от А.Г. Дьяконова
3. Дополнительно о других метриках можно почитать на [kaggle](https://www.kaggle.com/wiki/Metrics). К описанию каждой метрики добавлена ссылка на соревнования, где она использовалась
4. [Презентация](https://ld86.github.io/ml-slides/unbalanced.html#/) Богдана Мельника aka [ld86](https://habrahabr.ru/users/ld86/) про обучение на несбалансированных выборках

  

## Благодарности

  Спасибо [mephistopheies](https://habrahabr.ru/users/mephistopheies/) и [madrugado](https://habrahabr.ru/users/madrugado/) за помощь в подготовке статьи.