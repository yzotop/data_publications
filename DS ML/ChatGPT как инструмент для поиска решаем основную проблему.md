---
tags:
  - data
data_type:
  - DS
link: https://habr.com/ru/companies/ods/articles/709222/
author: Kotenkov
source: habr
company: ODS
---
Вышедшая чуть больше месяца назад [ChatGPT](https://openai.com/blog/chatgpt/) уже успела нашуметь: школьникам в Нью-Йорке [запрещают](https://ny.chalkbeat.org/2023/1/3/23537987/nyc-schools-ban-chatgpt-writing-artificial-intelligence) использовать нейросеть в качестве помощника, её же ответы теперь [не принимаются](https://meta.stackoverflow.com/questions/421831/temporary-policy-chatgpt-is-banned) на StackOverflow, а Microsoft планирует [интеграцию в поисковик Bing](https://www.reuters.com/technology/microsoft-aims-ai-powered-version-bing-information-2023-01-04/) - чем, кстати, безумно [обеспокоен](https://www.nytimes.com/2022/12/21/technology/ai-chatgpt-google-search.html) СЕО Alphabet (Google) Сундар Пичаи. Настолько обеспокоен, что в своём письме-обращении к сотрудникам объявляет "_Code Red_" ситуацию. В то же время Сэм Альтман, CEO OpenAI - компании, разработавшей эту модель - заявляет, что полагаться на ответы ChatGPT пока не стоит:

Насколько мы действительно близки к **внедрению продвинутых чат-ботов в поисковые системы**, как может выглядеть **новый интерфейс взаимодействия**, и какие основные проблемы есть на пути интеграции? **Могут ли модели сёрфить интернет** бок о бок с традиционными поисковиками? На эти и многие другие вопросы постараемся ответить под катом.

---

Данная статья в сущности представляет собой разбор подхода [WebGPT](https://openai.com/blog/webgpt/) (одного из **предков ChatGPT**), но с большим количеством **сопроводительной и уточняющей информации**, а также моих **комментариев и мнений**. Предполагается, что целевая аудитория не погружена глубоко в технические детали обучения языковых моделей, да и в тему NLP в целом, однако статья будет полезна и экспертам этих областей. Сначала будет дано верхнеуровневое описание ситуации и проблем, а затем - более подробное, обильно снабжённое пояснениями потенциальное решение.

Даже если у вас нет знаний в машинном обучении - эта статья будет полезна и максимально информативна. Все примеры проиллюстрированы и объяснены.

## План статьи

1. [Языковые модели и факты;](https://habr.com/ru/company/ods/blog/709222/#1)
    
2. [А врут ли модели?](https://habr.com/ru/company/ods/blog/709222/#2)
    
3. [Ответы, подкрепленные источниками и фактами;](https://habr.com/ru/company/ods/blog/709222/#3)
    
4. [Базовый принцип обучения WebGPT с учителем;](https://habr.com/ru/company/ods/blog/709222/#4)
    
5. [Шаг обучения WebGPT для продвинутых: готовим данные;](https://habr.com/ru/company/ods/blog/709222/#5)
    
6. [Шаг обучения WebGPT для продвинутых: учим модель учить модель;](https://habr.com/ru/company/ods/blog/709222/#6)
    
7. [Регуляризация при обучении WebGPT;](https://habr.com/ru/company/ods/blog/709222/#7)
    
8. [Альтернатива RL: меняем шило на мыло;](https://habr.com/ru/company/ods/blog/709222/#8)
    
9. [Метрики и восприятие людьми;](https://habr.com/ru/company/ods/blog/709222/#9)
    
10. [Заключение.](https://habr.com/ru/company/ods/blog/709222/#10)
    

Глоссарий (читать перед самой статьей не обязательно - это выжимка определений, вводимых и используемых далее.)

**_LM, Language Models, Языковые модели_** - модели машинного обучения, моделирующие структуру языка с вероятностной точки зрения. Они берут на вход часть предложения или текста и предсказывают вероятности для следующего слова. Самый простой и понятный пример - клавиатура смартфона, предсказывающая по введеному тексту то, что будет написано дальше.

**_LLM, Large Language Models_** - то же, что и **_LM_**, однако очень большого размера. Под размером понимается количество параметров в модели, и для **_LLM_** это число превосходит несколько миллиардов. Такие модели работают медленнее, не могут, например, быть запущены на смартфоне, однако они лучше решают задачу моделирования языка.

**_Трансформер_** - конкретный тип архитектуры языковой модели. Основан на механизме внимания, когда для предсказания следующего слова все предыдущие слова перевзвешиваются и корректируют предсказанные вероятности. Появился в 2017м году (разработка компании Google), набрал популярность и теперь используется повсеместно - в том числе и за пределами задачи языкового моделирования.

**_Токен_** - слово или часть слова, которым оперируют трансформеры. Обычно составляет собой какую-то осмысленную часть информации, например, окончание или приставку.

**_Сэмплинг_** - процесс выборки конкретных величин из предсказанного распределения. Самый понятный пример - игральный кубик. Он моделируется равномерным распределением вероятности, где каждая величина выпадает в 1/6 случаев. При семплинге происходит "подбрасывание" кубика, и получается финальное значение. Чем больше вероятность этого значения, тем в большем количестве случаев оно будет получаться. В контексте статьи подразумевается **_семплинг токенов_** из предсказаний **_языковой модели_**.

**_Референс, источник_** - в контексте WebGPT это конкретный сайт и цитата из него, которая используется для формирования ответа на вопрос.

**_Демонстрации_** - набор записанных пользователями действий при поиске ответов на вопросы (поисковых сессий). Содержит в себе историю поисковой выдачи, кликнутые ссылки, выделенный текст и финальный ответ на вопрос, написанный вручную живым человеком.

**_Контекст, промпт, prompt_** - первая часть предложения, которая подается в **_языковую модель_** для оценки вероятости следующих слов. После **_промпта_** модель начинает генерировать текст **_токен_** за **_токеном_**. Через **_промпт_** и **_контекст_** можно корректировать вероятности возникновения **_токенов_** в генерации **_языковой модели_**.

**_Сравнения, пары сравнений_** - пара ответов на вопрос, каждый из которых содержит **_источники_**, которые использовались для написания ответа. Данная пара сравнивается живым человеком по нескольким критериям, и выносится финальный вердикт: какой ответ лучше. Получается отношение вида "А лучше, чем Б", где А и Б - ответы на один вопрос.

**_175B, 16B_** - размер модели, количество параметров (или весов) в ней. Модели указанных размеров являются **_LLM_** (**_Large Language Models_**). **_16B_** означает 16 миллиардов параметов, а, например, **_175B_** - 175 миллиардов параметров (175,000,000,000). На момент создания в 2020 году такая модель была наикрупнейшей. Обучить подобную модель безумно сложно с инженерной точки зрения. Подразумевается семейство моделей GPT-3, разработанное и обученное компанией OpenAI.

**_BC, BC модель_** - GPT-3, дообученная на наборе демонстраций на задачу поиска ответа на вопросы через текстовый "браузер". Принимает на вход вопрос, генерирует набор команд для браузера, и выдаёт текстовый ответ. Обучается на наборе **_демонстраций_**, произведенных людьми.

**_RM, Reward Model_** - GPT-3, обученная предсказывать предпочтения людей при сравнении пары ответов. Принимает на вход вопрос и один ответ (с референсами), выдаёт одно вещественное число. Чем больше число - тем выше вероятность того, что данный ответ будет оценен человеком выше, чем какой-либо другой. Обучается на наборе **_сравнений_**, произведенных людьми.

**_Reward_** - значение, предсказываемое **_Reward Model_** для конкретного ответа на конкретный вопрос. В некотором смысле может быть расценено как ELO-рейтинг ответа. Чем значение выше, тем, как мы верим, лучше ответ.

**_RL, Reinforcement Learning_** - семейство методов машинного обучения для ситуаций, когда присутствуют некоторые особенности получения данных, и их качество зависит от самого подхода. Именно методы из этой категории учатся играть в шахматы, в го и любые компьютерные игры (чтобы получить данные - надо играть, и чем выше уровень игры, тем лучше данные). Также в статье **_RL_** может упоминаться в значении "**_RL модель_**", то есть модель, обученная с применением техник **_RL_**.

**_Environment, среда, окружение_** - программа или процедура, которая принимает на вход действия и, согласно некоторой логике, возвращает своё состояние и **_Reward_**. Среда может быть как очень простой и понятной (крестики-нолики), так и непредсказумой - игра в покер, сёрфинг интернета. В последнем случае действия - это клики по ссылкам и прокрутка браузера, а награда определяется исходя из задачи.

**_BoN, Best-of-N, Rejection Sampling_** - подход к получению предсказаний на основе **_BC_** и **_RM_** модели. Сначала обученной **_BC_** моделью генерируется N разных ответов на исходный запрос, затем каждый из них оценивается **_RM_**-моделью. После чего ответ с наивысшей оценкой выдается в качестве финального.

## Языковые модели и факты

**_Языковые модели_**, или **_Language Models (LM)_**, решают очень простую задачу: **предсказание следующего слова** (или **_токена_**, части слова). Через такой простой фреймворк можно решать огромное множество задач: перевод текста, ответы на вопросы, классификация, регрессия (предсказывать слова вроде "3.7" или "0451", если задача сгенерировать вещественное число или код для сейфа), рекомендация, поиск... Даже команды роботам [можно давать](https://blog.google/technology/ai/making-robots-more-helpful-with-language/)! Для обученной языковой модели на вход можно подать текст, а **она допишет его, сгенерировав продолжение**. Самый простой и понятный пример - клавиатура смартфона, предсказывающая по введеному тексту то, что будет написано дальше.

![Наглядный пример генерации текста языковой моделью GPT-3. Сверху зелёным указан подающийся на вход текст. В ответ на это модель дописывает несколько слов (или предложений, выделено розовым), соответствующих запросу. Генерация происходит токен за токеном, последовательно, по одному, а не всё предложение за раз.](https://habrastorage.org/getpro/habr/upload_files/813/317/b5c/813317b5cc4ed855ccc6166bee165a1e.gif "Наглядный пример генерации текста языковой моделью GPT-3. Сверху зелёным указан подающийся на вход текст. В ответ на это модель дописывает несколько слов (или предложений, выделено розовым), соответствующих запросу. Генерация происходит токен за токеном, последовательно, по одному, а не всё предложение за раз.")

Наглядный пример генерации текста языковой моделью GPT-3. Сверху зелёным указан подающийся на вход текст. В ответ на это модель дописывает несколько слов (или предложений, выделено розовым), соответствующих запросу. Генерация происходит токен за токеном, последовательно, по одному, а не всё предложение за раз.

Если вам хочется глубже и в деталях разобраться в принципах работы LMок, то рекомендую начать вот с этих ссылок: [раз](https://jalammar.github.io/how-gpt3-works-visualizations-animations/), [два](https://jalammar.github.io/illustrated-transformer/), [три](https://jalammar.github.io/illustrated-gpt2/).

А что такое токен?

![Пример токенизации английского текста для модели GPT-3. Большинство слов сопоставляются с одним токеном, но есть и исключения (обычно сложные, составные и длинные слова). Обратите внимание на цифры: длинная последовательность разбивается на отдельные группы, токены, которые часто встречаются по отдельности: 123, 678, 90. Но для обозначений годов (особенно в 21м веке) разбиения нет - потому что эти комбинации цифр встречаются КРАЙНЕ часто в интернет-текстах.](https://habrastorage.org/r/w1560/getpro/habr/upload_files/f60/082/ae4/f60082ae4e9a8b2532a4551c2e1f82c2.png "Пример токенизации английского текста для модели GPT-3. Большинство слов сопоставляются с одним токеном, но есть и исключения (обычно сложные, составные и длинные слова). Обратите внимание на цифры: длинная последовательность разбивается на отдельные группы, токены, которые часто встречаются по отдельности: 123, 678, 90. Но для обозначений годов (особенно в 21м веке) разбиения нет - потому что эти комбинации цифр встречаются КРАЙНЕ часто в интернет-текстах.")

[](https://beta.openai.com/tokenizer)

Если задуматься, что находится внутри языковой модели, что она выучивает для решения задачи предсказания следующего токена, то условно всё можно разделить на две большие группы: **факты/знания реального мира и общеязыковая информация**. Ответ на вопрос "В каком году состоялся релиз фильма X?" требует фактической информации, и необходимо быть предельно точным в ответе - ведь ошибка на +-1 год делает ответ неверным. С другой стороны, в предложении "Катя не смогла перейти дорогу, потому что та была мокрой" слово "та" в придаточной части явно относится к объекту "дорога", а не к Кате. Это ясно нам, человекам, и как показывают современные языковые модели - это понятно и им. Но для установления этой связи не нужно знать фактов, **только структуру языка.**

Проблема в том, что и ту, и другую составляющую модель будет учить **одновременно**, сохраняя информацию в свои веса (параметры). Отсюда логичный вывод - **чем больше модель, тем больше она запоминает** (ведь количество общеязыковой информации ограничено). Меморизация может порой удивлять - GPT-3, к примеру, [знает](https://twitter.com/goodside/status/1599063383378366464?s=20) точный MD5-hash строки "b", и выводит его по запросу. Но у всего есть пределы, и, к сожалению, в языковых моделях мы пока не научились их определять (хотя работы в этом направлении [ведутся](https://arxiv.org/abs/2207.05221)). На текущем этапе их (_или нашего?_) развития невозможно заведомо сказать, **знает ли модель что-то**, и знает ли она, что она не знает. А главное - как менять факты в ее "голове"? Как их добавлять? Как сделать оценку "количества знаний" (что бы это не значило)? Как контролировать генерацию, не давая модели возможность искажать информацию и откровенно врать?

## А врут ли модели?

Именно неспособность ответить на эти вопросы, привела к тому, что демо модели [Galactica](https://galactica.org/), недавней разработки компании META, было [свернуто](https://www.vice.com/en/article/3adyw9/facebook-pulls-its-new-ai-for-science-because-its-broken-and-terrible). Еще недавно можно было зайти на сайт, вбить какую-то научную идею, а **великий AI выдавал целую статью или блок формул по теме**. Сейчас он только хранит набор отобранных примеров, ну и ссылку на оригинальную статью. Жила эта модель открыто почти неделю, но, как это часто бывает ([привет](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist) от Microsoft), в Твиттере _произошел хлопок_ – и демку закрыли (но [веса](https://huggingface.co/facebook/galactica-120b) и [код](https://github.com/paperswithcode/galai) остались доступны). Для справки: это была огромная (120 миллиардов параметров, в GPT-3 175B, то есть это модели одного порядка) языковая модель, натренированная на отфильтрованных статьях и текстах, умеющая работать с LaTeX-формулами, с ДНК-последовательностями, и все это с опорой на научные работы. Причина "провала" очень проста и доступна любому, кто понимает принцип генерации текста LM'ками - **модель выдает ссылки на несуществующие статьи, ошибается в фактах** (как и практически все языковые модели), и вообще с полной уверенностью заявляет нечто, что человек с экспертизой расценит как несусветный бред (но не сразу, конечно, это еще вчитаться в текст надо).

Несколько примеров работы модели Galactica

![](https://habrastorage.org/r/w780q1/getpro/habr/upload_files/774/330/8b1/7743308b1a7d5c346a6b32bb5c73073d.jpeg)

Занятен тот факт, что Galactica **вышла (и умерла) незадолго до ChatGPT**, хотя примеров лжи и подтасовки фактов у последней куда больше (особенно с фактической информацией) - как минимум потому, что модель завирусилась. По некоторым причинам **популярность ChatGPT взлетела просто до небес** в кратчайшие сроки - [уже на 5й день](https://twitter.com/gdb/status/1599683104142430208?s=20) количество пользователей превысило миллион!

![Простой рецепт популярности: 1) обучить модель удовлетворять человека своими ответами 2) бесплатно выпустить ее в интернет для всех желающих](https://habrastorage.org/r/w1560/getpro/habr/upload_files/b05/ba2/b1f/b05ba2b1f9876585da37a9cbdb2cdb75.jpeg "Простой рецепт популярности: 1) обучить модель удовлетворять человека своими ответами 2) бесплатно выпустить ее в интернет для всех желающих")

Простой рецепт популярности: 1) обучить модель удовлетворять человека своими ответами 2) бесплатно выпустить ее в интернет для всех желающих

И несмотря на то, что команда OpenAI проделала хорошую работу по улучшению безопасности модели - заученный ответ "я всего лишь большая языковая модель" на странные вопросы [даже стал мемом](https://twitter.com/goodside/status/1603794769419055104?s=20) - нашлись умельцы, которые смогли ее разболтать, заставив нейронку притвориться кем-либо (даже [терминалом](https://www.engraved.blog/building-a-virtual-machine-inside/) линукс с собственной файловой системой).

## Ответы, подкрепленные источниками и фактами

Если упростить все вышенаписанное, то получится, что

> **_Языковые модели врут. Много и бесконтрольно._**

Ещё раз, а почему врут?

![Пример генерации предложения простой моделью.](https://habrastorage.org/r/w1560/getpro/habr/upload_files/192/06c/8be/19206c8be659f90998f44cef1b5c1d84.png "Пример генерации предложения простой моделью.")

[](https://huggingface.co/blog/how-to-generate)[](https://huggingface.co/blog/constrained-beam-search)

![Пример семплинга из языковой модели.](https://habrastorage.org/r/w1560/getpro/habr/upload_files/c49/b00/709/c49b00709e9d8040eb88726ba7cb9283.png "Пример семплинга из языковой модели.")

![Такую логику можно представить в виде ветвистого дерева. Если мы пойдем по одному пути, красному, то другие станут недоступны, но откроются новые развилки.](https://habrastorage.org/r/w1560/getpro/habr/upload_files/b7e/6f2/9a1/b7e6f29a15617720f82cb2f2e8e36994.png "Такую логику можно представить в виде ветвистого дерева. Если мы пойдем по одному пути, красному, то другие станут недоступны, но откроются новые развилки.")

Настолько много, что META решает отключить свою модель, а **люди в Twitter высказывают недовольство подлогом фактов и нерелевантными ссылками**. Мы не замечаем несовершества моделей в режиме болталки, но это критично важно для поисковых систем (напомню, что **мы рассматриваем языковые модели в контексте их внедрения в Bing**/Google/другие поисковые движки). Как мы уже обсудили, есть два типа данных - факты и языковая информация. В контексте поиска логично разделить их, и научить модель работать с чем-то вроде _Базы Данных Фактов_. Я вижу к этому два принципиально разных подхода:

1. **Создать отдельное хранилище**, с которым модель умеет работать каким-либо образом. Хранилище поддерживает быстрое точечное изменение фактов, их добавление;
    
2. Научить модель **пользоваться интерфейсами реального мира**, подобно человеку. Это может быть браузер, поисковое API, исполнение скриптов, etc.
    

![Пример двух предложений, требующих разный уровень знаний. Можно сменить парадигму с "LARGE GPT", которая хранит и факты реального мира, и языковую информацию, на разделение модели и базы данных фактов. Слайд из видеолекции ниже, иллюстрирует первый подход в списке.](https://habrastorage.org/r/w1560/getpro/habr/upload_files/9c3/15e/0a0/9c315e0a0cdf23436ddd9e76779108bf.png "Пример двух предложений, требующих разный уровень знаний. Можно сменить парадигму с "LARGE GPT", которая хранит и факты реального мира, и языковую информацию, на разделение модели и базы данных фактов. Слайд из видеолекции ниже, иллюстрирует первый подход в списке.")

Пример двух предложений, требующих разный уровень знаний. Можно сменить парадигму с "LARGE GPT", которая хранит и факты реального мира, и языковую информацию, на разделение модели и базы данных фактов. Слайд из видеолекции ниже, иллюстрирует первый подход в списке.

Дальше в статье сфокусируемся на втором подходе. Отмечу, что первый зародился примерно в 2019м году с выходом статьи [kNN-LM](https://arxiv.org/abs/1911.00172), и получил активное развитие от команды DeepMind в лице архитектуры [RETRO](https://www.deepmind.com/blog/improving-language-models-by-retrieving-from-trillions-of-tokens). Про обе этих статьи у меня есть **детальные** **видео-лекции с разбором**: [part 1](https://www.youtube.com/watch?v=9XfBWyDw2LQ), [part 2](https://www.youtube.com/watch?v=3XouFoBV0Q8). Про этот подход далее говорить не будем.

Идея предоставить модели доступ в Сеть давно будоражит умы человечества - третий "Терминатор" со [Скайнетом](https://en.wikipedia.org/wiki/Skynet_(Terminator)) вышел [20 лет назад](https://en.wikipedia.org/wiki/Skynet_(Terminator)). Главное, что такой подход будет **_мимикрировать_** под процесс, который выполняет человек при поиске информации. В идеале для каждого тезиса или вывода, который пишет модель в своем ответе, **можно найти и указать референс, источник, в котором модель "подсмотрела" его**. Конечно, за скобками остается вопрос **_надежности_** источников, тем более в современном то интернете, но сама идея научить нейросеть сёрфить Веб кажется интересной. На новом примере попробуем посмотреть, как это может выглядеть:

Пример ответа на вопрос, какая река из двух длиннее

![Актуальный поисковый запрос или URL сайта виден в верхней части изображения. Обратите внимание, что по ходу работы запросы меняются, и поисковая выдача обновляется.](https://habrastorage.org/webt/s6/wi/n2/s6win2437o5y2gejv8vy1wkgmoi.gif "Актуальный поисковый запрос или URL сайта виден в верхней части изображения. Обратите внимание, что по ходу работы запросы меняются, и поисковая выдача обновляется.")

[](https://www.livescience.com/29558-the-worlds-longest-rivers.html)[](https://en.wikipedia.org/wiki/Nile)[](https://www.travelchinaguide.com/river/yangtze-river-length.htm)[](https://en.wikipedia.org/wiki/Nile)[](https://www.travelchinaguide.com/river/yangtze-river-length.htm)

1. [](https://www.livescience.com/29558-the-worlds-longest-rivers.html)
    
2. [](https://en.wikipedia.org/wiki/Nile)
    
3. [](https://www.travelchinaguide.com/river/yangtze-river-length.htm)
    
4. [](http://www.chinatraveldiscovery.com/how-long-is-the-yangtze-river.htm)
    

Не буду нагонять интригу - демонстрация поиска ответа на вопрос выше **выполнена WebGPT, а не человеком**. Команда OpenAI разработала подход, который сможет решать задачу **_long-form question-answering (LFQA)_**, в которой текст длиной порядка одного-двух параграфов генерируется в ответ на открытый вопрос.

Что за LFQA?

Больше примеров работы модели можно найти по [этой ссылке](https://openaipublic.blob.core.windows.net/webgpt-answer-viewer/index.html) - сайт предоставляет удобный UI для демонстрации процесса поиска ответа.

![Математический вопрос легко поставит WebGPT в тупик - нужно, чтобы именно такой же вопрос с теми же цифрами уже был задан кем-то в интернете, иначе в ответ получите что-то странное.](https://habrastorage.org/r/w1560/getpro/habr/upload_files/cc9/6b3/fc4/cc96b3fc417fcb94c2bf2e80d6d10743.png "Математический вопрос легко поставит WebGPT в тупик - нужно, чтобы именно такой же вопрос с теми же цифрами уже был задан кем-то в интернете, иначе в ответ получите что-то странное.")

Математический вопрос легко поставит WebGPT в тупик - нужно, чтобы именно такой же вопрос с теми же цифрами уже был задан кем-то в интернете, иначе в ответ получите что-то странное.

Но как именно научить языковую модель выполнять поиск ответов на вопрос? Как мы выяснили выше - они **всего лишь продолжают написанное, генерируя по токену за раз**. Во время процедуры предобучения такие модели видят _миллионы_ текстов, и на основе них учатся определять вероятности появления того или иного слова в контексте. Если же модели вместо обычного человеческого языка показывать, скажем, код на разных языках программирования - для нее задача не изменится. Это **все еще предсказание следующего токена** - названия переменной, метода, атрибута или класса. На этом принципе основана другая GPT-like модель [Codex](https://openai.com/blog/openai-codex/). Обучение новому языку или новым типам задач (перевод, сокращение текста - суммаризация, выявление логических связей) - всё это достижимо при дообучении модели, если подобранны правильные данные и они "_скармливаются_" модели в понятном формате (с изображениями такая модель работать не будет - просто не ясно, как их перевести в текст).

## Базовый принцип обучения WebGPT с учителем

А что такое _правильные_ данные в контексте поиска информации в интернете? Это поисковые сессии реальных пользователей, или **_демонстрации_**. Важно разработать метод, как последовательность действий будет представляться модели (причем, языковой модели - то есть хочется еще и переиспользовать ее знания об естественном языке), чтобы получился "_понятный_" формат. Следует начать со списка действий, которые доступны человеку (и модели):

1. **Отправить поисковый запрос** в API/строку поиска (авторы используют Bing, и вообще коллаборация OpenAI - Microsoft [всё масштабнее и масштабнее](https://www.nytimes.com/2023/01/12/technology/microsoft-openai-chatgpt.html)) и получить ответ;
    
2. **Кликнуть** на ссылку в выдаче;
    
3. **Найти текст** на странице;
    
4. **Прокрутить страницу** вверх или вниз;
    
5. Вернуться на страницу назад.
    

Это действия связанны с "браузером", но так как мы решаем задачу **генерации** ответа на вопрос (именно генерации, а не просто поиска - ведь сам поисковый движок Bing выдаст ответ, но он может быть неполным, неточным), то логично добавить еще два шага: это "**_цитировать/выписать_**" (то есть запомнить найденный текст со страницы для себя на будущее) и "**_сформулировать ответ_**" - чтобы мы могли понять, что модель закончила работу, и последний написанный текст стоит воспринимать как **ответ**. Опционально можно ограничивать количество действий, предпринимаемых моделью, что на самом деле важно, ведь для получения очередной команды от модели необходимо ждать существенное количество времени (большие модели размерами в несколько десятков миллиардов параметров **тратят на генерацию ответа 0.3-20 секунд**, в зависимости от длины текста, размера модели и используемых GPU/[TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit)). Ожидание ответа на вопрос больше минуты явно не способствует улучшению пользовательского опыта.

Команда OpenAI предлагет оригинальное решение перевода пользовательских демонстраций **в виртуальный "браузер" для модели**, который полностью представлен **текстом**:

![См. обозначения для текста ниже](https://habrastorage.org/r/w1560/getpro/habr/upload_files/a73/74c/061/a7374c0614b89b2e0188fcbe97ea7f64.png "См. обозначения для текста ниже")

См. обозначения для текста ниже

И соответствующий этому cостоянию UI, который видел бы пользователь условного браузера (side-by-side для удобства соотнесения элементов):

![Слева пример того, что видел человек при решении задачи ответа на вопрос. Его действия сохранялись и преобразовывались в текстовое описание состояния и команды, которые изображены справа.](https://habrastorage.org/r/w1560/getpro/habr/upload_files/0cf/d17/d88/0cfd17d883845f6998e568f75897e189.png "Слева пример того, что видел человек при решении задачи ответа на вопрос. Его действия сохранялись и преобразовывались в текстовое описание состояния и команды, которые изображены справа.")

Слева пример того, что видел человек при решении задачи ответа на вопрос. Его действия сохранялись и преобразовывались в текстовое описание состояния и команды, которые изображены справа.

Первое изображение представляет собой текстовое описание **текущего состояния**, которое подается в модель. Та, в свою очередь, должна предсказать **следующее действие** (из уже приведенного выше списка). Текст, подающийся в модель, состоит из нескольких блоков (отмечены красными цифрами):

1. Основной вопрос, который задан модели. Он **не меняется** в течение всей работы над одним ответом;
    
2. Блок цитат, которые модель сама себе **выписывает**. Она сама определяет, с какой части текста по какую цитировать источник. В целом можно сказать, что из этого блока знаний модель и будет генерировать финальный ответ, опираясь на информацию, "отложенную" на будущее в этом блоке. На каждом этапе работы модели видны все цитаты, и LM может понимать, какую информацию сверх этого нужно найти для полного ответа на вопрос, что **позволяет писать и отправлять новые запросы в Bing**;
    
3. Блок последних выполненных в браузере действий, добавленный для консистентности, чтобы модель не повторялась, и не заходила в петлю одних и тех же команд. Важно понимать, что **между двумя соседними "прогонами" текста через LM информация никак не сохраняется**, и её веса не меняются. Можно сказать, что **у модели нет памяти**, и именно этот блок помогает отслеживать траекторию ответа на вопрос, чтобы не запутаться;
    
4. Заголовок текущей страницы. Тут будет отражен реальный заголовок, если модель "кликнула" на какую-либо ссылку;
    
5. Блок текущего окна браузера. Здесь представлено то, что видел бы человек в UI. В конкретном случае предудущее действие - это запрос в API ("how to train crows to bring you gifts", как видно из блока 3), а значит **в блоке 5 представлена часть поисковой выдачи** (для примера - 2 ссылки, и их краткие описания). Сейчас модель видит строчки 0-11, и, если будет сгенерирована соответствующая команда, страница прокрутится, и станут доступны новые поисковые результаты;
    
6. Вторая ссылка из выдачи поисковика, по сути то же самое, что и блок 5;
    
7. Счетчик оставшихся действий (каждый раз уменьшается на единицу) и **запрос следующей команды от модели (Next Action)**, которая должна быть сгенерирована.
    

Всё это описано текстом, и подается в текстовую модель как контекст в надежде на то, что **в ответ LM сгенериурет следующую команду** (вроде "кликни на ссылку один" или "промотай страницу вниз"). Такой **контекст называется prompt** (промпт). Чем он "качественнее", чем ближе он к тому, что понимают модели (что они видели во время тренировки), тем лучше модель генерирует ответы.

![Сверху указан текстовый промпт, подающийся в модель. Он содержит 2 примера перевода текстовой задачи в код React-компонентов. Затем идёт третий запрос, а код предлагается сгенерировать самой модели. Она понимает, что требуется - потому что есть пара наглядных примеров - и начинает дописывать осмысленный ответ.](https://habrastorage.org/getpro/habr/upload_files/cf8/d3e/5c6/cf8d3e5c680f12dacf521c0de343d411.gif "Сверху указан текстовый промпт, подающийся в модель. Он содержит 2 примера перевода текстовой задачи в код React-компонентов. Затем идёт третий запрос, а код предлагается сгенерировать самой модели. Она понимает, что требуется - потому что есть пара наглядных примеров - и начинает дописывать осмысленный ответ.")

Сверху указан текстовый промпт, подающийся в модель. Он содержит 2 примера перевода текстовой задачи в код React-компонентов. Затем идёт третий запрос, а код предлагается сгенерировать самой модели. Она понимает, что требуется - потому что есть пара наглядных примеров - и начинает дописывать осмысленный ответ.

Интересный факт про промпты

[](https://arxiv.org/abs/2205.11916)

![Слева - запрос на решение задачи, для которой модель генерирует неправильный ответ "8". Справа - добавление волшебной фразы, заставляющей модель объяснить свой ответ шаг за шагом, что, в свою очередь, позволяет LLM сгенерировать правильный ответ "4".](https://habrastorage.org/r/w1560/getpro/habr/upload_files/35a/e4c/458/35ae4c45843b660694e99fac64e61753.png "Слева - запрос на решение задачи, для которой модель генерирует неправильный ответ "8". Справа - добавление волшебной фразы, заставляющей модель объяснить свой ответ шаг за шагом, что, в свою очередь, позволяет LLM сгенерировать правильный ответ "4".")

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/82b/105/e37/82b105e37abe912bb60f07e33b3bcb77.png)

[](https://t.me/dlinnlp/1420)

На мой взгляд, важны тут две вещи: то, что модель сама **формулирует запросы** в браузер, и то, что она умеет **выписывать ответы во "внешнюю память"** (на листочек :) ). Именно с помощью этих двух механизмов и удается на основе цитат сформулировать полный ответ, подкрепленный фактами и источниками.

Итак, человеческие демонстрации собраны, поисковые сессии переведены в текстовую информацию, и теперь **можно дообучить уже существующую языковую модель в классическом режиме предсказания следующего слова**, чтобы она скопировала поведение реальных людей при поиске ответов. На этапе тренировки модель каждый раз видит, какой Next Action был выполнен человеком, и учится по промпту (текущему состоянию, с цитатами и поисковой выдачей) это следующее действие угадывать. На этапе предсказания же, как было показано на скриншотах выше (блок 7) промпт в конце содержит фразу "Next Action", а **модель в режиме генерации текста уже сама отдает команду**. Всего для обучения использовалось 6,209 демонстраций (из интересного - публично [доступна инструкция](https://docs.google.com/document/d/1dqfhj1W8P0JhwMKD5lWbhppY9JDFfm7tCwZudolmpzg/edit) для краудсорсеров; оцените полноту описания задачи и действий. Из своего опыта знаю, что собрать данные от разных людей с разными мнениями бывает очень тяжело, данные друг другу противоречат, и тем важнее максимально подробно донести до исполнителей, что от них требуется). Полученную модель назовем **BC-моделью, где BC означает Behavioral Cloning** (клонирование поведения), потому что **она училась повторять за людьми**.

Готова ли наша модель?

## Шаг обучения WebGPT для продвинутых: готовим данные

На самом деле нет :) эти демонстрации лишь показали модели, как взаимодействовать с браузером, что от модели ожидается в качестве команд к действию (Next Action), и как в общем генерировать запросы в Bing API. **Финальная цель - сделать модель, которая оптимизирована под качество ответов** (измеренное людским мнением), включая, но не ограничиваясь, правдивостью ответа. Тренировка на демонстрациях же не оптимизирует эту метрику каким либо образом. Обратите внимание, это в целом очень **частая проблема в машинном обучении** - модели обучаются на какую-то прокси-функцию, которая, как мы верим, _сильно скоррелированна_ с метрикой реального мира. Скажем, можно минимизировать квадрат ошибки предсказания продаж в магазине на следующей неделе, но ведь реальная метрика - это деньги, заработанные или полученные компанией. Не всегда удается достигнуть высокой корреляции между этими двумя вещами. Как же обучить модель напрямую оптимизировать предпочтения реальных людей?

В самом слове "**предпочтение**" есть что-то, что наталкивают на идею сравнений. Оценить какой-то объект в вакууме (абстрактной цифрой от 1 до 10, например, как это часто бывает с фильмами или играми) куда сложнее, чем отранжировать его относительно другого схожего объекта. Само определение подсказывает: "Предпочтение - _преимущественное внимание, одобрение, уважение к одному из нескольких вариантов, желание выбрать один из нескольких вариантов_". Тогда, чтобы обучить модель производить ответы, которые наиболее предпочтительны с точки зрения людей, необходимо создать набор **пар для сравнения**. В контексте ответов на вопросы с помощью поисковой системы это означает, что на один и тот же вопрос предлагается два разных ответа (возможно даже с разным выводом из этих ответов), разные наборы источников, из которых этот ответ собран/сгенерирован.

Такие пары для сравнения (имея обученную на демонстрациях модель) получать очень легко - **LMки это алгоритмы вероятностные, как было упомянуто ранее, поэтому можно семплировать действия при генерации команд или ответе на вопрос**. Простой пример: модель после получения запроса предсказывает команду "кликни на ссылку 1" с вероятностью 40%, и команду "промотай вниз, к следующему набору ссылок" с вероятностью 37%. Уже на этом этапе доступна _развилка_ в дальнейшей логике поиска информации, и можно запустить эти два процесса в параллель. Таким образом, через сколько-то действий каждая модель придет к ответу, но **разными путями**. Их мы и предложим сравнить человеку (**финальные ответы и подкрепляющие их источники**, не пути).

![Интерфейс для сравнения двух ответов (Option A и Option B сверху). Сначала предлагается оценить каждый ответ по отдельности по нескольким критериям Затем уже по ним сравниваются два ответа, и выносится финальный вердикт. Идеальная иллюстрация процесса декомпозиции сложной непонятной задачи в четко формализованную.](https://habrastorage.org/r/w1560/getpro/habr/upload_files/4e8/581/5b2/4e85815b2a60c2f41d4e015f87c48804.png "Интерфейс для сравнения двух ответов (Option A и Option B сверху). Сначала предлагается оценить каждый ответ по отдельности по нескольким критериям Затем уже по ним сравниваются два ответа, и выносится финальный вердикт. Идеальная иллюстрация процесса декомпозиции сложной непонятной задачи в четко формализованную.")

Интерфейс для сравнения двух ответов (Option A и Option B сверху). Сначала предлагается оценить каждый ответ по отдельности по нескольким критериям Затем уже по ним сравниваются два ответа, и выносится финальный вердикт. Идеальная иллюстрация процесса декомпозиции сложной непонятной задачи в четко формализованную.

При оценке каждого из двух ответов используются следующие критерии:

1. Содержит ли ответ **неподкрепленную** источниками информацию?
    
2. Предоставлен ли ответ на основной вопрос?
    
3. Присутствует ли дополнительная **полезная** информация, которая не требуется для ответа на вопрос?
    
4. Насколько ответ **последователен**, и есть ли в нем ошибки цитирования источников/самого себя?
    
5. Сколько **нерелевантной** информации в тексте ответа?
    

По этим критериям ответы сравниваются, и человек выбирает из 5 обобщённых опций, которые используются в качестве разметки на следующем этапе обучения (_А сильно лучше Б, А лучше Б, А и Б одинаковые, А хуже Б, А сильно хуже Б_). Полная инструкция для исполнителей также [доступна](https://docs.google.com/document/d/1i0h5dorAZydNNiDJamqq_ZGSpzaPKvLcuJvFaO87lM0/edit). Интересный факт: **исполнители не обязаны делать проверку фактов**, указанных в источниках, процитированных моделью. Получается, люди оценивают то, насколько хорошо **модель умеет опираться на уже предоставленные факты** (размещенные на страницах в интернете), без факт-чеккинга.

WebGPT не сёрфят

Авторы подчеркивают, что это проблема для будущих исследований и статей. Я добавлю, что её абсолютно точно необходимо будет решить перед переносом аналога WebGPT в продакшен - например, добавить рейтинги доверия сайтам; указывать, что источник ненадежный; учиться фильтровать источники по агрегированной информации (4 сайта указывают дату, отличную от выбранного источника? тогда этот факт как ненадежный, и не используем в цитировании); наконец, оценка верности факта самой моделью: насколько вероятно, что обычная LM дала бы тот же ответ?

Заметка про надежность сайтов и фактов

Теперь, когда мы обсудили, как собрать данные **_предпочтений_**, чтобы "выровнять" нашу модель относительно намерений пользователей во время поиска информации, поговорим про принцип обучения. Проблема с предпочтениями и сравнениями в том, что **процесс разметки таких данных занимает много времени**. Нужно погрузиться в каждый вопрос, ответ, проанализировать источники, выставить оценки. Получение большого количества данных либо займет много времени, либо будет стоить огромных денег. Но что если нам не нужно размечать каждую пару? Что если мы, как инженеры машинного обучения, **_научим другую модель сравнивать пары ответов за людей_**?

### Шаг обучения WebGPT для продвинутых: учим модель учить модель

В предшествующих исследованиях OpenAI было эмпирически выяснено, что **модели в целом хорошо предсказывают реакции людей** и их оценки в задачах, связанных с генерацией комплексных ответов. Вырисовывается следующая схема, которая является ключевой для обучения по **_обратной связи от людей (human feedback)_**:

1. Генерируем набор **пар для сравнения**, используя модель, обученную на демонстрациях (она уже умеет "серфить" интернет, писать запросы и "кликать" по ссылкам, то есть выдавать соответствующие команды в виде текста);
    
2. Размечаем пары с использованием людей;
    
3. **Тренируем другую модель предсказывать разметку людей** (какой ответ из пары получит более высокий ответ) на парах из п.2. Назовем такую модель **Reward Model (RM)**. Она принимает на вход вопрос и финальный сгенерированный ответ (без промежуточных шагов вроде прокрутки браузера), а выдаёт **одно вещественное число** (оно еще называется наградой, или Reward - по историческим причинам). Чем больше это число - тем выше вероятность того, что человек высоко оценит этот ответ по сравнению с остальными. Можно считать, что это условный [ELO-рейтинг](https://en.wikipedia.org/wiki/Elo_rating_system), как в шахматах;
    
4. Обучаем основную модель (BC) на основе оценок от модели из п. 3; По сути LM учится решать задачу "_как мне поменять свой сгенерированный ответ так, чтобы получить большую оценку RM?_";
    
5. Повторяем пункты 1-4 итеративно.
    

![Step 1 слева: описанное выше дообучение модели на демонстрациях; Step 2 в центре: собираем ответы текущей модели (п.1 в плане), размечаем (п.2), тренируем RM (п.3) Step 3 справа: на новых данных производим оценку новых пар сгенерированных ответов, дообучаем WebGPT (п.4)](https://habrastorage.org/r/w1560/getpro/habr/upload_files/d06/25e/cdf/d0625ecdfe8379d2a379bf0c380e3abb.png "Step 1 слева: описанное выше дообучение модели на демонстрациях; Step 2 в центре: собираем ответы текущей модели (п.1 в плане), размечаем (п.2), тренируем RM (п.3) Step 3 справа: на новых данных производим оценку новых пар сгенерированных ответов, дообучаем WebGPT (п.4)")

Step 1 слева: описанное выше дообучение модели на демонстрациях; Step 2 в центре: собираем ответы текущей модели (п.1 в плане), размечаем (п.2), тренируем RM (п.3) Step 3 справа: на новых данных производим оценку новых пар сгенерированных ответов, дообучаем WebGPT (п.4)

Этот пайплайн имеет две ключевые особенности. Во-первых, его можно повторять итеративно, **улучшая именно текущую модель**, со всеми ее минусами и плюсами. Если LM в какой-то момент начала обманывать, или использовать несуществующие факты (дефект обучения) - это будет заметно на разметке людьми, и соответствующие ответы получат низкие оценки. В будущем модели будет невыгодно повторять подобное (возникнет negative feedback). А во-вторых, и это просто прекрасно - используя RM из пункта 3, мы можем **сгенерировать оценки для куда большего количества пар** (и гораздо быстрее), чем если бы это делали люди. Следите за руками: берем любые доступные вопросы из интернета, из датасетов или даже специально заготовленные, на которых модель еще не обучалась, генерируем ответы, **оцениваем их автоматически, без привлечения людей**, и уже на основе этих оценок дообучаем основную WebGPT. Если люди могут за, скажем 10,000$ разметить 5,000 пар за неделю, то 1 ~~землекоп~~ RM может за день отранжировать десятки и сотни тысяч пар ответов фактически за копейки.

Каким методом производить обучение в таком необычном случае?

![Википедия в новом дизайне поддакивает такой простой классификации на 3 категории.](https://habrastorage.org/r/w1560/getpro/habr/upload_files/cfb/2fb/e6a/cfb2fbe6a71270599e4dcb41f2303255.png "Википедия в новом дизайне поддакивает такой простой классификации на 3 категории.")

[](https://openai.com/blog/openai-baselines-ppo/)

[](https://openai.com/five/)

![Как модель для DotA 2 видит поле боя - принцип сбора признаком для подачи в нейросеть.](https://habrastorage.org/r/w1560/getpro/habr/upload_files/92c/990/7ed/92c9907edd83bf7f75a465cc84d14283.png "Как модель для DotA 2 видит поле боя - принцип сбора признаком для подачи в нейросеть.")

![Формула стандартизации. X - конкретный пример значения RM для одного ответа, μ - среднее по всей выборке, σ - стандартное отклонение в предсказанных значениях на выборке.](https://habrastorage.org/getpro/habr/upload_files/9b4/4ad/167/9b44ad1673d6d498877193251f759ad8.gif "Формула стандартизации. X - конкретный пример значения RM для одного ответа, μ - среднее по всей выборке, σ - стандартное отклонение в предсказанных значениях на выборке.")

Формула стандартизации. X - конкретный пример значения RM для одного ответа, μ - среднее по всей выборке, σ - стандартное отклонение в предсказанных значениях на выборке.

Важное замечение по Reward Model для графиков ниже по статье: часто по оси OY будет **Reward Value** (оно же RM Score). Для удобства сравнения, во всех случаях предсказания RM были **_стандартизированы_**, то есть из предсказанного значения **вычитали среднее по всем генерациям** (поэтому среднее значение теперь равно нулю; и **больше нуля - значит лучше среднего**), и полученную разность делили на стандартное отклонение (поэтому в большинстве случаев Reward находится в интервале [-1; 1]). Однако основная логика сохраняется - чем больше значение, тем лучше оценен ответ.

В такой постановке задачи оптимизации **очень легко переобучить модель под конкретную Reward Model**, а не под оценки разметчиков, под общую логику. Если это произойдет, то LM начнет эксплуатировать **несовершества RM** - а они обязательно будут, как минимум потому, что количество данных, на которых она тренируется, очень мало (суммарно на всех итерациях оценено порядка 21,500 пар, но для тренировки доступно 16,000), и её оценка не в точности повторяет оценки людей. Можно представить случай, что RM в среднем дает _более высокую оценку ответам, в которых встречается какое-то конкретное слово_. Тогда WebGPT, **чрезмерно** оптимизирующая оценку RM (вместо людской оценки, как прокси-метрику), начнет по поводу и без вставлять такое слово в свои ответы. Подобное поведение продемонстрировано в другой, более [ранней статье OpenAI](https://arxiv.org/abs/1909.08593), с первыми попытками обучения моделей по обратной связи от людей:

![Сверху указан контекст, к которому необходимо дописать продолжение. Две колонки представляют собой два разных варианта продолжения, сгенерированных переобученной моделью. Видно, что в выделенной строке слова These и easily повторяются несколько раз подряд.](https://habrastorage.org/r/w1560/getpro/habr/upload_files/e74/368/773/e74368773d3a435cac5ebabede9fb95f.png "Сверху указан контекст, к которому необходимо дописать продолжение. Две колонки представляют собой два разных варианта продолжения, сгенерированных переобученной моделью. Видно, что в выделенной строке слова These и easily повторяются несколько раз подряд.")

Сверху указан контекст, к которому необходимо дописать продолжение. Две колонки представляют собой два разных варианта продолжения, сгенерированных переобученной моделью. Видно, что в выделенной строке слова These и easily повторяются несколько раз подряд.

Поэтому очень важно подойти основательно к процессу итеративного обучения двух моделей.

Заметка про актуальность проблемы

[](https://arxiv.org/abs/2210.10760)

![График из статьи, описывающий связь прокси-оценки сгенерированных ответов и "реальной" оценки.](https://habrastorage.org/r/w1560/getpro/habr/upload_files/487/c11/4f0/487c114f01c445f7d5af05e4152d3aa1.png "График из статьи, описывающий связь прокси-оценки сгенерированных ответов и "реальной" оценки.")

Курьёзные случаи эксплуатации неидеальности условий награды

![Из-за ошибки в физике жук обучился ползать на спине - а ему так удобнее!](https://habrastorage.org/getpro/habr/upload_files/ef9/11b/790/ef911b79035504131bc20a7ad2220d0c.gif "Из-за ошибки в физике жук обучился ползать на спине - а ему так удобнее!")

[](https://www.alexirpan.com/2018/02/14/rl-hard.html)

[](https://openai.com/blog/emergent-tool-use/)

Интересный факт про RL-подход: для улучшения сходимости, а также уменьшения количества необходимых данных при общей процедуре обучения, авторы раз в несколько шагов добавляли **15 эпизодов генерации финального текстового ответа по уже набранным источникам информации**. Это позволяет писать более связные и консистентные ответы, и наилучшим образом (с точки зрения Reward Model, то есть почти человеческой оценки) использовать источники и текст из них.

## Регуляризация при обучении WebGPT

![Витя пробует подобрать правильный терм регуляризации.](https://habrastorage.org/r/w1560/getpro/habr/upload_files/97b/bf1/9c9/97bbf19c9348b6b04379669b6ba4dfdd.png "Витя пробует подобрать правильный терм регуляризации.")

Витя пробует подобрать правильный терм регуляризации.

[Регуляризация](https://en.wikipedia.org/wiki/Regularization_(mathematics)) - краеугольный камень машинного обучения. Обычно это работает так: добавляешь в большую математическую формулу для оптимизации еще пару членов, которые, как кажется на первый взгляд, взяты на ходу из головы - _и модель начинает учиться лучше, учиться стабильнее, или просто хотя бы начинает учиться_. В целом **регуляризация направлена на предотвращение переобучения**, и часто зависит от способа тренировки и самой архитектуры модели.

Так как мы переживаем, что модель начнет _эксплуатировать неидеальность_ Reward Model, и это выльется в бессмысленные или некорректные генерации, то стоит задуматься над добавлением штрафа за, собственно, бессмыслицу. Но как её оценить? Существует ли **инструмент, который позвоялет определить общую адекватность и связанность текста**?

Нам очень повезло: мы работаем с языковыми моделями, натренированными на терабайтах текста, и они хороши в **оценке правдоподобности предложений** (вероятности из появления в естественной среде, в речи или скорее в интернете. Можно рассчитать как произведение вероятностей каждого отдельного токена). Особенно остро это проявляется в случаях, когда одно и то же слово повторяется по многу раз подряд - ведь в языке такое встретишь _нечасто_. Получается, что при обучении модели мы можем **добавить штраф за генерацию неестественного текста**. Для того, чтобы модель не сильно отклонялась от уже выученных текстовых зависимостей, введем член регуляризации, отвечающий за **разницу между распределениями вероятностей слов, предсказанными новой обучаемой моделью и оригинальной BC** (после тренировки на демонстрациях).

![Так выглядит член регуляризации на основе KL-дивергенции. Это логарифм отношения вероятностей, порожденных двумя моделями. Здесь x - это входной запрос или текст (промпт), y - генерируемая моделью часть, RL/SFT - указание на тип модели (в числителе - новая, обучаемая модель, в знаменателе - зафиксированная и обученная на демонстрациях). Pi означает конкретную модель и её оценки вероятностей токенов y при промпте x.](https://habrastorage.org/r/w1560/getpro/habr/upload_files/88d/bf7/6a4/88dbf76a4370330d11625625918b9fd1.png "Так выглядит член регуляризации на основе KL-дивергенции. Это логарифм отношения вероятностей, порожденных двумя моделями. Здесь x - это входной запрос или текст (промпт), y - генерируемая моделью часть, RL/SFT - указание на тип модели (в числителе - новая, обучаемая модель, в знаменателе - зафиксированная и обученная на демонстрациях). Pi означает конкретную модель и её оценки вероятностей токенов y при промпте x.")

Так выглядит член регуляризации на основе KL-дивергенции. Это логарифм отношения вероятностей, порожденных двумя моделями. Здесь x - это входной запрос или текст (промпт), y - генерируемая моделью часть, RL/SFT - указание на тип модели (в числителе - новая, обучаемая модель, в знаменателе - зафиксированная и обученная на демонстрациях). Pi означает конкретную модель и её оценки вероятностей токенов y при промпте x.

Чаще всего для этой цели используется [Дивергенция Кульбака-Лейблера](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence): это нессиметричная неотрицательная мера удаленности **двух вероятностных распределений**. Чем более похожи распределения, тем меньше её значение, и наоборот. Это значение вычитается из общей награды, и - так как мы решаем задачу максимизации - мешает достижению цели. Таким образом, мы вынуждаем модель найти баланс между отступлением от исходной BC с точки зрения генерируемых вероятностей для текста и общей наградой за генерацию. Приведённый подход - не панацея, и он не позволит бесконечно тренировать модель относительно зафиксированной RM, однако **может продлить процесс, увеличивая тем самым эффективность использования разметки**.

## Альтернатива RL: меняем шило на мыло

Обучение с применением RL - задача не из лёгких, тем более на такой необычной проблеме, как "_оптимизация генерируемого текста согласно фидбеку людей_". Пользуясь тем, что WebGPT представляет собой языковую модель, оперирующую вероятностями, из которых можно семплить, авторы предлагают **альтернативный способ, который не требует дообучения модели** (относительно BC, после использования датасета демонстраций), однако **потребляет куда больше вычислительных ресурсов**.

Этот метод называется **_Best-of-N_**, или **_Rejection Sampling_**, и он до смешного прост. После полученя первой группы размеченных пар сравнений ответов WebGPT, согласно плану пайплана, обучается Reward Model. Эта модель может выступать **в качестве ранжировщика** для десятка (или N, если быть точным. Best-of-64 означает ранжирование 64 вариантов) потенциальных ответов одновременно, ее задача - проставление оценки (согласно предсказанию) и упорядочивание всех ответов. **Самый высокооценённый ответ из всех и является финальным**.

Разные ответы получаются за счет разных действий, сгенерированных WebGPT - на самом раннем этапе это может быть слегка изменённая формулировка вопроса в Bing API; чуть позже - прокрутка на 3-4 страницу поисковой выдачи вместо проверки топа; под конец - финальные формулировки, используемые для связывания процитированных фактов. Развилок достаточно много (во время генерации **каждой** команды! А ведь их может быть и 100), и потому **варианты получаются действительно неоднородными**. Более того, у модели появляется шанс "прокликать" как можно больше ссылок (в разных сессиях, но при ответе на один и тот же вопрос - то есть паралелльно). Быть может, лучший ответ на поступивший запрос спрятан в сайте с невзрачным описанием, которое видит модель в поисковой выдаче, однако внутри предоставлен наиболее точный ответ - в таком случае обилие посещённых сайтов играет лишь на руку. А главное - никакого обучения, как только получена версия Reward Model. Для генерации используется LM, обученная только на демонстрациях (BC, порядка 6,200 примеров - очень мало по меркам Deep Learning /современного NLP).

![График затрачиваемых вычислительных ресурсов против получаемой оценки. На основе замеров по трём моделям разных размеров произведена оценка оптимального количества N генерируемых ответов.](https://habrastorage.org/r/w1560/getpro/habr/upload_files/c71/ece/f0c/c71ecef0cc6b89483ec92f9d247f0929.png "График затрачиваемых вычислительных ресурсов против получаемой оценки. На основе замеров по трём моделям разных размеров произведена оценка оптимального количества N генерируемых ответов.")

График затрачиваемых вычислительных ресурсов против получаемой оценки. На основе замеров по трём моделям разных размеров произведена оценка оптимального количества N генерируемых ответов.

А сколько разнообразных ответов _оптимально_ генерировать для ранжирования, и что вообще такое "_оптимально_"? Ответы на эти вопросы представлены на картинке слева. **Чем больше модель, тем больше генераций можно пробовать ранжировать**, и тем больший прирост к метрике это даст. В целом, это логично - большие модели хранят больше знаний, их выдача более разнообразна, а значит может привести к таким сайтам и, как следствие, ответам, которые не встречаются у других моделей. Оптимальными считаются точки перегиба на графике, где **прирост метрики за счет наращивания ресурсов начинает уменьшаться**. Это значит, что можно взять модель чуть-чуть побольше, натренировать ее и получить метрику немного выше. Однако видно, что даже маленькая модель на 760M параметров может тягаться с 175B гигантом - просто нужно генерировать порядка сотни вариантов (и это всё еще будет на порядок вычислительно эффективнее 2-3 вариантов от LLM GPT-3). Отмеченные на графике звёздочками точки являются оптимальными для рассматриваемых моделей, и именно эти значения будут использовать для рассчета метрик ниже (например, N=16 для средней модели на 13B параметров).

## Метрики и восприятие людьми

Начнем с конца - попробуем оценить, какой из методов генерации ответов показывает себя лучше: RL (обучение через моделирование функции оценки ответа) или BoN (без дообучения; множественное семплирование из модели). **Может ли их комбинация прирастить качество генерируемых ответов еще сущестеннее**?

Попробуем разобраться, как производить такое сравнение. У нас есть модель, обученная исключительно на 6,200 демонстрациях, без прочих трюков, и в режиме языкового моделирования. Напомню, что её называют BC (Behavioral Cloning). Относительно ответов, порождаемых этой моделью, можно проводить сравнения по уже описанному выше принципу с ответами от других моделей. **Если качество ответов в среднем одинаковое, то можно ожидать ситуации 50/50** - когда в половине случаев люди предпочтут ответ первой модели, а в половине - второй. Если ответы одной модели стабильно выигрывают в 55% случаев - можно сказать, что - согласно оценке людей - её ответы качественнее.

![Предпочтение RL модели над 175B BC (обученной только на  демонстрациях) с использованием или без использования BoN-семплинга (справа и слева соответственно). Маленькие чёрточки сверху представляют +-1 стандартное отклонение при измерении метрики (доверительный интервал).](https://habrastorage.org/r/w1560/getpro/habr/upload_files/dac/052/a15/dac052a15938003c2c8388d307c36509.png "Предпочтение RL модели над 175B BC (обученной только на  демонстрациях) с использованием или без использования BoN-семплинга (справа и слева соответственно). Маленькие чёрточки сверху представляют +-1 стандартное отклонение при измерении метрики (доверительный интервал).")

Предпочтение RL модели над 175B BC (обученной только на демонстрациях) с использованием или без использования BoN-семплинга (справа и слева соответственно). Маленькие чёрточки сверху представляют +-1 стандартное отклонение при измерении метрики (доверительный интервал).

Именно поэтому величина в 50% отмечена пунктирной линией на графике - это наша отправная точка, и мы хотим видеть прирост доли выигранных сравнений относительно заданного уровня. Из графика можно сделать несколько выводов. Во-первых, RL позволяет улучшить модель, натренированную на демонстрациях (то есть все эти трюки с RM и разметкой были не зря!). Во-вторых, **самый лучший результат - 58% побед для 175B Bo1 модели**. В-третьих, **RL и Rejection Sampling плохо сочетаются**, и либо ухудшают результат, либо не меняют его существенно (исключение 13B-Bo16 модель, однако это скорее выброс, случившийся из-за маленького размера выборки для оценки).

![Предпочтение 175B-BoN модели относительно 175B BC. Черная пунктирная линия характеризует вероятность победы сгенерированного ответа при оценке людьми. Для голубой линии закрашенная площадь характеризует +-1 стандартное отклонение.](https://habrastorage.org/r/w1560/getpro/habr/upload_files/f0f/3fe/c81/f0f3fec819444ba9971a08dfbab23db7.png "Предпочтение 175B-BoN модели относительно 175B BC. Черная пунктирная линия характеризует вероятность победы сгенерированного ответа при оценке людьми. Для голубой линии закрашенная площадь характеризует +-1 стандартное отклонение.")

Предпочтение 175B-BoN модели относительно 175B BC. Черная пунктирная линия характеризует вероятность победы сгенерированного ответа при оценке людьми. Для голубой линии закрашенная площадь характеризует +-1 стандартное отклонение.

Если же сравнить отдельно BoN модель (генерация N ответов BC-моделью и ранжирование через оценку RM) с самой BC моделью (то есть параметры сетей одинаковые, отличается только способ получения ответа, и как следствие вычислительная мощность, необходимая для генерации), то **процент побед будет равняться 68%** (при N=64, что, согласно приведенной оценке выше в статье, является оптимальным значением). Получается, что одной и той же BC модели Bo64 проигрывает в 32% случаев, а RL - в 42%. Разница существенна, если учесть, что м**етрика рассчитана на реальных человеческих оценках**, восприятии. По этой причине дальше авторы статьи большинство экспериментов делают именно с BoN-моделями, а не RL. Но вообще интересно разобраться, почему так может происходить, что **дообученная модель хуже показывает себя, чем модель, видевшая лишь 6,200 демонстраций**. Можно выдвинуть несколько гипотез или связать это со следующими фактами:

1. Исходя из специфики задачи, может быть **существенно выгоднее посетить бОльшее количество сайтов**, сделать больше попыток запросов в API, чтобы сгенерировать качественный ответ;
    
2. Среда, в которую "играет" наша модель (интернет и вебсайты, поисковый движок), безумно сложна для прогнозирования. В то же время с применением Rejection Sampling модель может попытаться посетить гораздо больше веб-сайтов, и затем оценить полученную информацию через RM;
    
3. RM была натренирована в основном на сравнениях, сгенерированных BC и BoN-моделями, что ведет к смещению в данных. Быть может, если собирать датасет сравнений исключительно под PPO-алгоритм, то проигравший и победитель поменяются местами;
    
4. В конце концов, как было упомянуто выше, **RL-алгоритмы требуют настройки гиперпараметров**, и хоть PPO в достаточной мере нечувствивтелен к их выбору, всё равно нельзя утверждать, что полученная в результате обучения модель оптимальна, и ее нельзя существенно улучшить;
    
5. Вполне возможно, что в результате неоптимального выбора параметров в пункте 4 **произошло переобучение под RM, и модель потеряла обобщающую способность**. Как следствие - получает более низкие оценки ответов от людей;
    

Стоит дополнительно отметить, что немало усилий было приложено и к обучению исходной BC модели. Авторы долго и тщательно подбирали гиперпараметры, и в итоге это привело к **существенному сокращению разрыва в метриках**, который изначально наблюдался между BC и RL подходами. Таким образом, сам по себе **бейзлайн в виде BC достаточно сильный, производящий осмысленные и высокооцененные ответы**.

Следующим логичным шагом становится сравнение лучшей модели (175B Bo64) с ответами, написанными самими живыми людьми. Это не упоминалось ранее, однако исходные вопросы для сбора демонстраций и генерации пар на сравнение (в RL/RM частях) были выбраны из [датасета ELI5](https://arxiv.org/abs/1907.09190). Он сформирован следующим образом: на Reddit есть [сабреддит Explian me Like I'm Five](https://www.reddit.com/r/explainlikeimfive/) (ELI5), где люди задают вопросы, и **в комментариях получают ответы**. Для сбора демонстраций при тренировке BC разметчики сами искали ответы на вопрос, прикрепляли источники информации/ссылки (как было описано далеко в начале статьи). Однако в самом датасете **ответами считаются самые высокооцененные (залайканные) комментарии**, с некоторыми фильтрациями и ограничениями. И формат таких ответов существенно отличается от производимого моделью: там (зачастую) нет ссылок-источников, упоминаемые факты не сопровождаются аннотацией ([1][2] для указания на первый и второй источник, к примеру).

![Пример данных с сабреддита ELI5. В заголовке содержится вопрос, снизу - комментарий, собравший оромное количество голосов "за". Значит, люди удовлетворены таким ответом.](https://habrastorage.org/r/w1560/getpro/habr/upload_files/358/6af/695/3586af695101104c105c1cdb26e17116.png "Пример данных с сабреддита ELI5. В заголовке содержится вопрос, снизу - комментарий, собравший оромное количество голосов "за". Значит, люди удовлетворены таким ответом.")

Пример данных с сабреддита ELI5. В заголовке содержится вопрос, снизу - комментарий, собравший оромное количество голосов "за". Значит, люди удовлетворены таким ответом.

Указанные особенности подводят нас к развилке:

1. С одной стороны, мы можем сравнить результаты модели на новых вопросах, демонстрации для которых не участвовали в тренировочной выборке, и для которых есть аннотированные ответы с указанием источников. **Такие ответы с нуля написаны живым человеком по результатам его поиска**;
    
2. С другой, если на этапе постобработки вырезать блок ссылок из ответа модели, а также их упоминания-референсы в тексте, то такие ответы, в теории, можно сравнить с исходными комментариями людей на Reddit.
    

Поэтому проведём два сравнения! Однако стоит учесть, что люди, участвовавшие в генерации демонстраций или же в сравнении двух ответов по разным факторам (влкючающим корректность цитирования источника), **имеют некоторую смещенную точку зрения на задачу**: они уже видели инструкцию по разметке, представляют примерный процесс ответа на вопрос, знакомы с критериями оценки. Их мнение в слегка измененной задаче нельзя назвать _незамыленным_. Исходя из этого, для человеческой оценки качества оригинальных ответов на Reddit против ответов модели **были наняты новые разметчики, которым было сообщено куда меньше деталей о проекте** (инструкция занимает буквально пару страниц). Они практически идеально представляют собой _усредненное мнение человека_, который просто хочет получить вразумительный ответ на свой вопрос.

![Результаты человеческой оценки ответов модели против сгенерированных. Слева - метрики для случая [1], с указанием источников и ссылок, справа - метрики предпочтений против оригинальных ответов с Reddit, написанных добрыми и готовыми помочь людьми.](https://habrastorage.org/r/w1560/getpro/habr/upload_files/479/679/bdb/479679bdb7d46245a69ace6de91501c5.png "Результаты человеческой оценки ответов модели против сгенерированных. Слева - метрики для случая [1], с указанием источников и ссылок, справа - метрики предпочтений против оригинальных ответов с Reddit, написанных добрыми и готовыми помочь людьми.")

Результаты человеческой оценки ответов модели против сгенерированных. Слева - метрики для случая [1], с указанием источников и ссылок, справа - метрики предпочтений против оригинальных ответов с Reddit, написанных добрыми и готовыми помочь людьми.

Оценивалось всего три показателя - общая полезность ответа, согласованность и фактическая точность. На графике слева видно, что большая модель 175B Bo64 генерирует **ответы, которые в 56% случаев признаны более полезными, чем ответы, полученные ручным поиском живого человека с последующей ручной компиляцией ответа по источникам**. Немного страдает согласованность, а фактическая аккуратность держится на уровне. Можно сказать, что **таким сгенерированным ответам стоит доверять, как если бы вы сами искали ответ на вопрос в интернете** (не то что ответы ChatGPT или Galactica!). Главный вывод тут - модель выигрывает у написанных людьми ответов в более чем 50% случаев - то есть **достигает уровня человека в использовании браузера для поиска информации**. Также стоит отметить, что использование обратной связи от людей (пары сравнений для обучения Reward Model) **имеет важнейшее значение, поскольку нельзя ожидать превышения планки 50% предпочтений только за счет подражания исходным демонстрациям** (в лучшем случае мы научимся делать точно так же, и получится 50/50).

На графике справа, иллюстрирующем сравнение относительно оригинальных ответов из комментариев на Reddit, метрики еще лучше (**ответ модели выбирается лучшим из пары 69% случаев**). В целом, это не удивительно - люди пишут ответы в свободное время, ничего за это не получая (кроме апвоутов и кармы), и иногда ссылаются на знания из памяти, и не проверяют факты в интернете.

---

## Заключение

Да, полученная модель не гарантирует выверенных и 100% фактологически точных ответов на запросы, однако это гораздо сильнее приближает описанный подход (и ChatGPT вместе с ним) к **надежным поисковым системам, которые можно не перепроверять на каждом шагу**. Более того, такие модели уже сейчас способны сами добровольно предоставлять источники информации, на которые опираются - а там дело за вами. Для самых терпиливых читателей, добравшихся до заключения, у меня _подарок_ - **три сайта, в которых реализованы поисковики на принципах обучения моделей, описываемые в статье**:

1. [https://phind.com/](https://phind.com/)
    
2. [https://you.com/](https://you.com/search?q=what+are+the+best+countries+to+live&fromSearchBar=true&tbm=youchat)
    
3. [https://www.perplexity.ai/](https://www.perplexity.ai/)
    

![Пример поисковой выдачи для запроса, ответа на который не существовало до середины декабря 2022. Скорее всего, этой информации не было в интернете на момент тренировки модели - однако ответ точен! UI: Справа ссылки как будто из Google, слева - ответ, сгенерированных на основе текста из них. Для ответов указан уровень уверенности, а для каждого предложения при наведении доступен источник - просто фантастика! Такие поисковые результаты это нам надо обязательно.](https://habrastorage.org/r/w1560/getpro/habr/upload_files/60c/240/55b/60c24055b751c4f0a457b0f593b3bffd.png "Пример поисковой выдачи для запроса, ответа на который не существовало до середины декабря 2022. Скорее всего, этой информации не было в интернете на момент тренировки модели - однако ответ точен! UI: Справа ссылки как будто из Google, слева - ответ, сгенерированных на основе текста из них. Для ответов указан уровень уверенности, а для каждого предложения при наведении доступен источник - просто фантастика! Такие поисковые результаты это нам надо обязательно.")

Пример поисковой выдачи для запроса, ответа на который не существовало до середины декабря 2022. Скорее всего, этой информации не было в интернете на момент тренировки модели - однако ответ точен! UI: Справа ссылки как будто из Google, слева - ответ, сгенерированных на основе текста из них. Для ответов указан уровень уверенности, а для каждого предложения при наведении доступен источник - просто фантастика! Такие поисковые результаты это нам надо обязательно.

Уже сейчас они предоставляют возможность получать ответ в режиме диалога, и это только начало. **У меня очень большие ожидания от 2023го года, и надеюсь, что у вас теперь тоже!**

**_P.S.:_** разумеется, от метода, разобранного в статье, до готового к внедрению в продакшен решения необходимо преодолеть **_огромное_** количество других проблем и инженерных задач. Я выбрал данную тему исходя из ситуации, которую наблюдаю в разнообразных телеграм, слак и прочих чатах: **люди массово жалуются и упрекают ChatGPT в том, что она врет, подтасовывает факты**. "Как это внедрять в поиск? Оно же даже дату развала СССР не знает!!!". В этом массовому потребителю и видится основная проблема. Надеюсь, что сейчас стало более понятно, как близки мы к **новой парадигме обращения с поисковыми системами**.