---
tags:
  - data
data_type:
  - AB tests
link: https://habr.com/ru/companies/retailrocket/articles/261593/
source: habr
company: RetailRocket
author: RetailRocket
---
«Горячая» и часто обсуждаемая сегодня тема оптимизации конверсии привела к безусловной популяризации А/Б-тестирования, как единственного объективного способа узнать правду о работоспособности тех или иных технологий/решений, связанных с увеличением экономической эффективности для онлайн-бизнеса.  
  
За [этой популярностью](http://www.ngdata.com/top-ab-testing-blogs/) скрывается практически полное отсутствие культуры в организации, проведении и анализе результатов экспериментов. В [Retail Rocket](http://retailrocket.ru/?utm_source=habr-blog&utm_medium=referral&utm_campaign=habr-ab-tests) мы накопили большую экспертизу в оценке экономической эффективности от систем персонализации в электронной коммерции. За два года был отстроен идеальный процесс проведения A/Б-тестов, которым мы и хотим поделиться в рамках этой статьи.  
  

## Два слова о принципах A/Б-тестирования

  
В теории все невероятно просто:  

- Выдвигаем гипотезу о том, что какое-то изменение (например, [персонализация главной страницы](http://retailrocket.ru/blog/keys-personalizatsii-glavnoy-stranitsyi-internet-magazina-www-dostavka-ru/)) увеличит конверсию интернет-магазина.
- Создаем альтернативную версию сайта «Б» — копию исходной версии «А» с изменениями, от которых мы ждем роста эффективности сайта.
- Всех посетителей сайта случайным образом делим на две равные группы: одной группе показываем исходный вариант, второй — альтернативный.
- Одновременно измеряем конверсию для обеих версий сайта.
- Определяем статистически достоверно победивший вариант.

  

![image](https://habrastorage.org/r/w1560/files/f93/400/fba/f93400fbacc24a21833d3606b8619908.png)

  
Прелесть этого подхода в том, что любую гипотезу можно проверить с помощью цифр. Нет необходимости спорить или опираться на мнение псевдоэкспертов. Запустили тест, замерили результат, перешли к следующему тесту.  
  

## Пример в цифрах

  
Для примера представим, что мы внесли на сайт какое-то изменение, запустили А/Б-тест и получили следующие данные:  
  

![image](https://habrastorage.org/r/w1560/files/b2e/f53/d1f/b2ef53d1f2f24ec296662ad2221eee26.jpg)

  
Конверсия — не статичная величина, в зависимости от количества “испытаний” и “успехов” (в случае интернет-магазина — посещений сайта и оформленных заказов соответственно) конверсия распределяется в определенном интервале с расчетной вероятностью.  
  

![image](https://habrastorage.org/r/w1560/files/210/20f/8a9/21020f8a9eba474eac92a30038490788.jpg)

  
Для таблицы выше это означает, что если мы приведем еще 1000 пользователей на версию сайта “А” при неизменных внешних условиях, то с вероятностью 99% эти пользователи сделают от 45 до 75 заказов (то есть сконвертируются в покупателей с коэффициентом от 4.53% до 7.47%).  
  
Сама по себе эта информация не слишком ценная, однако, при проведении А/Б-теста мы можем получить 2 интервала распределения конверсии. Сравнение пересечения так называемых “доверительных интервалов” конверсий, получаемых от двух сегментов пользователей, взаимодействующих с разными версиями сайта, позволяет принять решение и утверждать, что один из тестируемых вариантов сайта статистически достоверно превосходит другой. Графически это можно представить так:  

![image](https://habrastorage.org/r/w1560/files/913/b74/643/913b74643d5641e7b97c05b47ad712b5.png)

  

## Почему 99% ваших А/Б-тестов проводятся неверно?

  
Итак, с вышеописанной концепцией проведения экспериментов знакомо уже большинство, про неё рассказывают на отраслевых мероприятиях и пишут статьи. В [Retail Rocket](http://retailrocket.ru/?utm_source=habr-blog&utm_medium=referral&utm_campaign=habr-ab-tests) одновременно проходит 10-20 А/Б-тестов, за последние 3 года мы столкнулись с огромным количеством нюансов, которые часто остаются без внимания.  
  
В этом есть огромный риск: если А/Б-тест проводится с ошибкой, то бизнес гарантированно принимает неверное решение и получает скрытые убытки. Более того, если вы ранее проводили A/Б-тесты, то скорее всего они были проведены некорректно.  
  
Почему? Разберем самые частые ошибки, с которыми мы сталкивались в процессе проведения множества пост-тест анализов результатов проведенных экспериментов при внедрении [Retail Rocket](http://retailrocket.ru/?utm_source=habr-blog&utm_medium=referral&utm_campaign=habr-ab-tests) в интернет-магазины наших клиентов.  
  

### Доля аудитории в тесте

  
Пожалуй самая распространенная ошибка — при запуске тестирования не проверяется, что вся аудитория сайта участвует в нем. Довольно частый пример из жизни (скриншот из Google Analytics):  
  

![image](https://habrastorage.org/r/w1560/files/530/ec7/825/530ec78250e741008973344c020496d9.png)

  
На скриншоте видно, что суммарно в тесте приняло участие чуть менее 6% аудитории. Крайне важно, чтобы вся аудитория сайта относилась к одному из сегментов теста, в противном случае невозможно оценить влияние изменения на бизнес в целом.  
  

### Равномерность распределения аудитории между тестируемыми вариациями

  
Недостаточно распределить всю аудиторию сайта по сегментам теста. Также важно проделать это равномерно по всем срезам. Рассмотрим на примере одного из наших клиентов:  
  

![image](https://habrastorage.org/r/w1560/files/b73/6ad/3be/b736ad3be0e048a1b871093697a12f5e.png)

  
Перед нами ситуация, при которой аудитория сайта делится неравномерно между сегментами теста. В данном случае в настройках инструмента для тестирования было выставлено деление трафика 50/50. Такая картина — явный признак того, что инструмент для распределения трафика работает не так, как ожидалось.  
  
Кроме того, обратите внимание на последний столбец: видно, что во второй сегмент попадает больше повторной, а значит и более лояльной аудитории. Такие люди будут совершать больше заказов и исказят результаты тестирования. И это ещё один признак некорректной работы инструмента тестирования.  
  
Чтобы исключить подобные ошибки через несколько дней после запуска тестирования всегда проверяйте равномерность деления трафика по всем доступным срезам (как минимум по городу, браузеру и платформе).  
  

### Фильтрация сотрудников интернет-магазина

  
Следующая распространенная проблема связана с сотрудниками интернет-магазинов, которые, попав в один из сегментов теста, оформляют заказы, поступившие по телефону. Тем самым сотрудники формируют дополнительные продажи в одном сегменте теста, в то время как звонящие находятся во всех. Безусловно, подобное аномальное поведение в конечном счете исказит итоговые результаты.  
  
Операторов call-центра можно выявить с помощью отчета по сетям в Google Analytics:  
  

![image](https://habrastorage.org/r/w1560/files/42a/8fe/3ce/42a8fe3ce8ec4995828fc4387a9e8421.png)

  
  
![image](https://habrastorage.org/r/w1560/files/fc2/772/2e7/fc27722e7cb541f19c689946ccc89e37.png)На скриншоте пример из нашего опыта: посетитель 14 раз заходил на сайт из сети под названием «Торговый центр электроники на Пресне» и 35 раз оформил заказ — это явное поведение сотрудника магазина, который по какой-то причине оформлял заказы через корзину на сайте, а не через панель администратора магазина.  
  
В любом случае, всегда можно выгрузить заказы из Google Analytics и присвоить им свойство “оформлен оператором” или “оформлен не оператором”. После чего построить сводную таблицу как на скриншоте, отражающую другую ситуацию, с которой мы сталкиваемся довольно часто: если взять выручку сегмента RR и Not RR (“сайт с Retail Rocket” и “без” соответственно), то “сайт с Retail Rocket” приносит меньше денег, чем “без”. Но если выделить заказы, оформленные операторами call-центра, то окажется, что Retail Rocket дает прирост выручки на 10%.  
  

### На какие показатели стоит обращать внимание при финальной оценке результатов?

  
В прошлом году проводился А/Б-тест, результаты которого оказались следующими:  

- +8% к конверсии в сегменте “сайт с Retail Rocket”.
- Средний чек практически не изменился (+0.4% — на уровне погрешности).
- Рост по выручке +9% в сегменте “Сайт с Retail Rocket”.

  
После составления отчетов результатов мы получили такое письмо от клиента:  
  

![image](https://habrastorage.org/r/w1560/files/8f0/64b/798/8f064b798948400e9c4062c92cdeca98.png)

  
Менеджер интернет-магазина настаивал на том, что если средний чек не изменился, то эффекта от сервиса нет. При этом полностью игнорируется факт общего прироста выручки благодаря системе рекомендаций.  
  
Так на какой показатель стоит ориентироваться? Конечно, для бизнеса самое важное — это деньги. Если в рамках A/Б-теста трафик делился между сегментами посетителей равномерно, то нужный показатель для сравнения — выручка (revenue) по каждому сегменту.  
  
В жизни ни один инструмент для случайного деления трафика не дает абсолютно равных сегментов, всегда существует отличие в доли процента, поэтому необходимо нормировать выручку по количеству сессий и использовать метрику “выручка на посещение” (revenue per visit).  
  
Это признанный в мире KPI, на который рекомендуем ориентироваться при проведении A/Б-тестов.  
  
Важно помнить, что выручка от оформленных на сайте заказов и “исполненная” выручка (выручка от фактически оплаченных заказов) — это абсолютно разные вещи.  
  
Вот пример А/Б-теста, в котором система [Retail Rocket](http://retailrocket.ru/?utm_source=habr-blog&utm_medium=referral&utm_campaign=habr-ab-tests) сравнивалась с другой рекомендательной системой:  
  

![image](https://habrastorage.org/r/w1560/files/155/7d7/8b4/1557d78b43b04c97b79703bdeb622f36.jpg)

  
Сегмент «не Retail Rocket» побеждает по всем параметрам. Однако, в рамках следующего этапа пост-тест анализа были исключены заказы call-центра, а также аннулированные заказы. Результаты:  
  

![image](https://habrastorage.org/r/w1560/files/7fd/191/c6b/7fd191c6b60d4251aca36c03a6a45180.jpg)

  
Пост-тест анализ результатов — обязательный пункт при проведении A/Б-тестирования!  
  

### Срезы данных

  
Работа с разными срезами данных — крайне важная составляющая при пост-тест анализе.  
Вот еще один кейс тестирования Retail Rocket на одном из крупнейших интернет-магазинов в России:  
  

![image](https://habrastorage.org/r/w1560/files/60c/0f3/ad7/60c0f3ad78114c6fb3348909e0caba19.png)

  
На первый взгляд мы получили отличный результат — прирост выручки +16.7%. Но если добавить в отчет дополнительный срез данных _«Тип устройства»_ можно увидеть следующую картину:  
  

![image](https://habrastorage.org/r/w1560/files/0b5/773/996/0b57739962ab44b186fb2c785fca612b.png)

  

1. По Desktop-трафику рост выручки практически 72%!
2. На планшетах в сегменте Retail Rocket просадка.

  
Как выяснилось после тестирования, на планшетах некорректно отображались блоки рекомендаций Retail Rocket.  
  
Очень важно в рамках пост-тест анализа строить отчеты как минимум в разрезе города, браузера и платформы пользователя, чтобы не упустить подобные проблемы и максимизировать результаты.  
  

### Статистическая достоверность

  
Следующая тема, которую необходимо затронуть, — статистическая достоверность. Принимать решение о внедрении изменения на сайт можно только после достижения статистической достоверности превосходства.  
  
Для подсчета статистической достоверности конверсии существует множество online-инструментов, например, [htraffic.ru/calc/](http://htraffic.ru/calc/):  

![image](https://habrastorage.org/r/w1560/files/572/388/0dc/5723880dc01746c8aaca6950a7aaf325.png)

  
Но конверсия — это не единственный показатель, определяющий экономическую эффективность сайта. Проблема большинства А/Б-тестов сегодня — проверяется только статистическая достоверность конверсии, что является недостаточным.  
  

### Средний чек

  
Выручка интернет-магазина строится из конверсии (доли людей, которые покупают) и из среднего чека (размера покупки). Статистическую достоверность изменения среднего чека посчитать сложнее, но без этого никак, иначе неизбежны неверные выводы.  
  
На скриншоте продемонстрирован очередной пример A/Б-теста Retail Rocket, при котором в один из сегментов попал заказ на сумму более миллиона рублей:  
  

![image](https://habrastorage.org/r/w1560/files/d11/414/d86/d11414d861f046be9de75c514b7388b0.png)

  
Этот заказ составляет почти 10% от всей выручки за период проведения теста. В таком случае, при достижении статистической достоверности по конверсии можно ли считать достоверными результаты по выручке? Конечно, нет.  
  
Такие огромные заказы значительно искажают результаты, у нас есть два подхода к пост-тест анализу с точки зрения среднего чека:  

1. Сложный. «[Байесовская статистика](https://ru.wikipedia.org/wiki/%D0%91%D0%B0%D0%B9%D0%B5%D1%81%D0%BE%D0%B2%D1%81%D0%BA%D0%B0%D1%8F_%D0%B2%D0%B5%D1%80%D0%BE%D1%8F%D1%82%D0%BD%D0%BE%D1%81%D1%82%D1%8C)», о которой мы расскажем в следующих статьях. В [Retail Rocket](http://retailrocket.ru/?utm_source=habr-blog&utm_medium=referral&utm_campaign=habr-ab-tests) мы используем ее для оценки достоверности среднего чека внутренних тестов по оптимизации алгоритмов рекомендаций.
2. Простой. Отсечение нескольких процентилей заказов сверху и снизу (как правило, 3-5%) списка, отсортированного по убыванию суммы заказа.

  

### Время проведения теста

  
И напоследок, всегда обращайте внимание на то, когда вы запускаете тест и как долго он длится. Старайтесь не запускать тест за несколько дней до крупных гендерных праздников и в праздничные/выходные дни. Сезонность также прослеживается на уровне получения зарплат: как правило, это стимулирует продажи дорогих товаров, в частности, электроники.  
  
Кроме того, существует доказанная зависимость между средним чеком в магазине и временем принятия решения до покупки. Проще говоря, чем дороже товары — тем дольше их выбирают. На скриншоте пример магазина, в котором 7% пользователей до покупки размышляют от 1 до 2 недель:  
  

![image](https://habrastorage.org/r/w1560/files/72b/691/a72/72b691a724e74e689b6c12b2a5641299.png)

  
Если на таком магазине проводить А/Б-тест меньше недели, то в него не попадет около 10% аудитории и влияние изменения на сайте на бизнес однозначно оценить невозможно.  
  

## Вместо вывода. Как провести идеальный А/Б-тест?

  
Итак, для исключения всех описанных выше проблем и проведения правильного A/Б-теста нужно выполнить 3 шага:  
  
    **1. Разделить трафик 50 / 50**  
_Сложно:_ с помощью балансировщика трафика.  
_Просто:_ воспользоваться open source библиотекой [Retail Rocket Segmentator](https://github.com/RetailRocket/RetailRocket.Segmentator), которую поддерживает команда Retail Rocket. За несколько лет тестов нам не удалось решить описанные выше проблемы в инструментах вроде Optimizely или Visual Website Optimizer.  
  
**Цель на первом шаге:**  

- Получить равномерное распределение аудитории по всем доступным срезам (браузеры, города, источники трафика и т.д.).
- 100% аудитории должно попасть в тест.

  
   **2. Провести А/А-тест**  
Не меняя ничего на сайте, передавать в Google Analytics (или другую систему веб-аналитики, которая вам нравится) разные идентификаторы сегментов пользователей (в случае с Google Analytics — Custom var / Custom dimension).  
  
**Цель на втором шаге:** не получить победителя, т.е. в двух сегментах с одинаковыми версиями сайта не должно быть разницы по ключевым показателям.  
  
   **3. Провести пост-тест анализ**  

- Исключить сотрудников компании.
- Отсечь экстремальные значения.
- Проверить значимость ценности конверсии, использовать данные по исполняемости и аннуляции заказов, т.е. учесть все кейсы, упомянутые выше.

  
**Цель на последнем шаге:** принять правильное решение.  
  
Делитесь своими кейсами проведения A/Б-тестов в комментариях!