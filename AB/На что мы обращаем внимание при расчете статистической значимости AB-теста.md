---
link: https://habr.com/ru/companies/uchi_ru/articles/500918/
source: habr
tags:
  - data
data_type:
  - AB tests
company: Uchi
author: Uchi
---
В Учи.ру мы стараемся даже небольшие улучшения выкатывать A/B-тестом, только за этот учебный год их было больше 250. A/B-тест — мощнейший инструмент тестирования изменений, без которого сложно представить нормальное развитие интернет-продукта. В то же время, несмотря на кажущуюся простоту, при проведении A/B-теста можно допустить серьёзные ошибки как на этапе дизайна эксперимента, так и при подведении итогов. В этой статье я расскажу о некоторых технических моментах проведения теста: как мы определяем срок тестирования, подводим итоги и как избегаем ошибочных результатов при досрочном завершении тестов и при тестировании сразу нескольких гипотез.  
![](https://habrastorage.org/r/w1560/webt/gb/fg/n9/gbfgn9rfqwkmzgh4-dadqomhvu0.png)  
  
Типичная схема A/B-тестирования у нас (да и у многих) выглядит так:  
  

1. Разрабатываем фичу, но перед раскаткой на всю аудиторию хотим убедиться, что она улучшает целевую метрику, например, вовлечённость.
2. Определяем срок, на который запускается тест.
3. Случайно разбиваем пользователей на две группы.
4. Одной группе показываем версию продукта с фичей (экспериментальная группа), другой — старую (контрольная).
5. В процессе мониторим метрику, чтобы вовремя прекратить особо неудачный тест.
6. По истечении срока теста сравниваем метрику в экспериментальной и контрольной группах.
7. Если метрика в экспериментальной группе статистически значимо лучше, чем в контрольной, раскатываем протестированную фичу на всех. Если же статистической значимости нет, завершаем тест с отрицательным результатом.

  
Всё выглядит логично и просто, дьявол, как всегда, в деталях.  
  

## Статистическая значимость, критерии и ошибки

  
В любом A/B-тесте присутствует элемент случайности: метрики групп зависят не только от их функционала, но и от того, какие пользователи в них попали и как они себя ведут. Чтобы достоверно сделать выводы о превосходстве какой-то группы, нужно набрать достаточно наблюдений в тесте, но даже тогда вы не застрахованы от ошибок. Их различают два типа:  
  

- Ошибка первого рода происходит, если мы фиксируем разницу между группами, хотя на самом деле её нет. В тексте также будет встречаться эквивалентный термин — ложноположительный результат. Статья посвящена именно таким ошибкам.
- Ошибка второго рода происходит, если мы фиксируем отсутствие разницы, хотя на самом деле она есть.

  
При большом количестве экспериментов важно, чтобы вероятность ошибки первого рода была мала. Её можно контролировать с помощью статистических методов. Например, мы хотим, чтобы в каждом эксперименте вероятность ошибки первого рода не превышала 5% (это просто удобное значение, для собственных нужд можно брать другое). Тогда мы будем принимать эксперименты на уровне значимости 0.05:  
  

1. Есть A/B-тест с контрольной группой A и экспериментальной — B. Цель — проверить, что группа B отличается от группы A по какой-то метрике.
2. Формулируем нулевую статистическую гипотезу: группы A и B не отличаются, а наблюдаемые различия объясняются шумом. По умолчанию всегда считаем, что разницы нет, пока не доказано обратное.
3. Проверяем гипотезу строгим математическим правилом — статистическим критерием, например, критерием Стьюдента.
4. В результате получаем величину p-value. Она лежит в диапазоне от 0 до 1 и означает вероятность увидеть текущую или более экстремальную разницу между группами при условии верности нулевой гипотезы, то есть при отсутствии разницы между группами.
5. Значение p-value сравнивается с уровнем значимости 0.05. Если оно больше, принимаем нулевую гипотезу о том, что различий нет, иначе считаем, что между группами есть статистически значимая разница.

  
Проверить гипотезу можно параметрическим или непараметрическим критерием. Параметрические опираются на параметры выборочного распределения случайной величины и обладают большей мощностью (реже допускают ошибки второго рода), но предъявляют требования к распределению исследуемой случайной величины.  
  
Самый распространенный параметрический тест — критерий Стьюдента. Для двух независимых выборок (случай A/B-теста) его иногда называют критерием Уэлча. Этот критерий работает корректно, если исследуемые величины распределены нормально. Может показаться, что на реальных данных это требование почти никогда не удовлетворяется, однако на самом деле тест требует нормального распределения выборочных средних, а не самих выборок. На практике это означает, что критерий можно применять, если у вас в тесте достаточно много наблюдений (десятки-сотни) и в распределениях нет совсем уж длинных хвостов. При этом характер распределения исходных наблюдений неважен. Читатель самостоятельно может убедиться, что критерий Стьюдента работает корректно даже на выборках, сгенерированных из распределений Бернулли или экспоненциального.  
  
Из непараметрических критериев популярен критерий Манна — Уитни. Его стоит применять, если ваши выборки очень малого размера или есть большие выбросы (метод сравнивает медианы, поэтому устойчив к выбросам). Также для корректной работы критерия в выборках должно быть мало совпадающих значений. На практике нам ни разу не приходилось применять непараметрические критерии, в своих тестах всегда пользуемся критерием Стьюдента.  
  

## Проблема множественного тестирования гипотез

  
Самая очевидная и простая проблема: если в тесте кроме контрольной группы есть несколько экспериментальных, то подведение итогов с уровнем значимости 0.05 приведёт к кратному росту доли ошибок первого рода. Так происходит, потому что при каждом применении статистического критерия вероятность ошибки первого рода будет 5%. При количестве групп ![$ngroups$](https://habrastorage.org/getpro/habr/formulas/e83/e44/0ec/e83e440ec20380092afd5d5dc19e9b49.svg) и уровне значимости ![$\alpha$](https://habrastorage.org/getpro/habr/formulas/234/756/ba0/234756ba02fcf44c798e4e66aedb3efd.svg) вероятность, что какая-то экспериментальная группа выиграет случайно, составляет:  

![$P(any\ false\ positive) = 1 − (1 − \alpha) ^ {ngroups}$](https://habrastorage.org/getpro/habr/formulas/605/53b/c0f/60553bc0f11edca45d2826f17d37d54f.svg)

  
Например, для трёх экспериментальных групп получим 14.3% вместо ожидаемых 5%. Решается проблема поправкой Бонферрони на множественную проверку гипотез: нужно просто поделить уровень значимости на количество сравнений (то есть групп) и работать с ним. Для примера выше уровень значимости с учётом поправки составит 0.05/3 = 0.0167 и вероятность хотя бы одной ошибки первого рода составит приемлемые 4.9%.  
  

**Метод Холма — Бонферрони**

  
Строго говоря, сравнения групп по разным метрикам или срезам аудитории тоже подвержены проблеме множественного тестирования. Формально учесть все проверки довольно сложно, потому что их количество сложно спрогнозировать заранее и подчас они не являются независимыми (особенно если речь идёт про разные метрики, а не срезы). Универсального рецепта нет, полагайтесь на здравый смысл и помните, что если проверить достаточно много срезов по разным метрикам, то в любом тесте можно увидеть якобы статистически значимый результат. А значит, надо с осторожностью относиться, например, к значимому приросту ретеншена пятого дня новых мобильных пользователей из крупных городов.  
  

## Проблема подглядывания

  
Частный случай множественного тестирования гипотез — проблема подглядывания (peeking problem). Смысл в том, что значение p-value по ходу теста может случайно опускаться ниже принятого уровня значимости. Если внимательно следить за экспериментом, то можно поймать такой момент и ошибочно сделать вывод о статистической значимости.  
  
Предположим, что мы отошли от описанной в начале поста схемы проведения тестов и решили подводить итоги на уровне значимости 5% каждый день (или просто больше одного раза за время теста). Под подведением итогов я понимаю признание теста положительным, если p-value ниже 0.05, и его продолжение в противном случае. При такой стратегии доля ложноположительных результатов будет пропорциональна количеству проверок и уже за первый месяц достигнет 28%. Такая огромная разница кажется контринтуитивной, поэтому обратимся к методике A/A-тестов, незаменимой для разработки схем A/B-тестирования.  
  
Идея A/A-теста проста: симулировать на исторических данных много A/B-тестов со случайным разбиением на группы. Разницы между группами заведомо нет, поэтому можно точно оценить долю ошибок первого рода в своей схеме A/B-тестирования. На гифке ниже показано, как изменяются значения p-value по дням для четырёх таких тестов. Равный 0.05 уровень значимости обозначен пунктирной линией. Когда p-value опускается ниже, мы окрашиваем график теста в красный. Если бы в этом время подводились итоги теста, он был бы признан успешным.  
![](https://habrastorage.org/webt/x6/vg/ha/x6vghamdqyssbgock-z0bkpzn4i.gif)  
  
Рассчитаем аналогично 10 тысяч A/A-тестов продолжительностью в один месяц и сравним доли ложноположительных результатов в схеме с подведением итогов в конце срока и каждый день. Для наглядности приведём графики блуждания p-value по дням для первых 100 симуляций. Каждая линия — p-value одного теста, красным выделены траектории тестов, в итоге ошибочно признанных удачными (чем меньше, тем лучше), пунктирная линия — требуемое значение p-value для признания теста успешным.  
![](https://habrastorage.org/r/w1560/webt/of/pe/3j/ofpe3jhjnemmlf2swowulr7jlbu.png)  
  
На графике можно насчитать 7 ложноположительных тестов, а всего среди 10 тысяч их было 502, или 5%. Хочется отметить, что p-value многих тестов по ходу наблюдений опускались ниже 0.05, но к концу наблюдений выходили за пределы уровня значимости. Теперь оценим схему тестирования с подведением итогов каждый день:  
![](https://habrastorage.org/r/w1560/webt/98/bs/ms/98bsmsxbs1hknrac33vg9mawe4a.png)  
  
Красных линий настолько много, что уже ничего не понятно. Перерисуем, обрывая линии тестов, как только их p-value достигнут критического значения:  
![](https://habrastorage.org/r/w1560/webt/jk/3k/__/jk3k__q2qr2vxrtj43fhkhdojs8.png)  
  
Всего будет 2813 ложноположительных тестов из 10 тысяч, или 28%. Понятно, что такая схема нежизнеспособна.  
  
Хоть проблема подглядывания — это частный случай множественного тестирования, применять стандартные поправки (Бонферрони и другие) здесь не стоит, потому что они окажутся излишне консервативными. На графике ниже — доля ложноположительных результатов в зависимости от количества тестируемых групп (красная линия) и количества подглядываний (зелёная линия).  
  
![](https://habrastorage.org/r/w1560/webt/up/ji/yg/upjiyg_axaouzzhhj9dpasel86y.png)  
  
Хотя на бесконечности и в подглядываниях мы вплотную приблизимся к 1, доля ошибок растёт гораздо медленнее. Это объясняется тем, что сравнения в этом случае независимыми уже не являются.  
  

**Байесовский подход и проблема подглядывания**

  

## Методы досрочного завершения теста

  
Есть варианты тестирования, позволяющие досрочно принять тест. Расскажу о двух из них: с постоянным уровнем значимости (поправка Pocock’a) и зависимым от номера подглядывания (поправка O'Brien-Fleming’a). Строго говоря, для обеих поправок нужно заранее знать максимальный срок теста и количество проверок между запуском и окончанием теста. Причём проверки должны происходить примерно через равные промежутки времени (или через равные количества наблюдений).  
  

### Pocock

  
Метод заключается в том, что мы подводим итоги тестов каждый день, но при сниженном (более строгом) уровне значимости. Например, если мы знаем, что сделаем не больше 30 проверок, то уровень значимости надо выставить равным 0.006 (подбирается в зависимости от количества подглядываний методом Монте-Карло, то есть эмпирически). На нашей симуляции получим 4% ложноположительных исходов — видимо, порог можно было увеличить.  
![](https://habrastorage.org/r/w1560/webt/1g/k1/-y/1gk1-yay5upf9rg7fgdlotuk-ua.png)  
  
Несмотря на кажущуюся наивность, некоторые крупные компании пользуются именно этим способом. Он очень прост и надёжен, если вы принимаете решения по чувствительным метрикам и на большом трафике. Например, в «Авито» по умолчанию [уровень значимости принят за 0.005](https://habr.com/ru/company/avito/blog/454164/).  
  

### O'Brien-Fleming

  
В этом методе уровень значимости изменяется в зависимости от номера проверки. Надо заранее определить количество шагов (или подглядываний) в тесте и рассчитать уровень значимости для каждого из них. Чем раньше мы пытаемся завершить тест, тем более жёсткий критерий будет применён. Пороговые значения статистики Стьюдента ![$Z_{step}$](https://habrastorage.org/getpro/habr/formulas/fc3/6dc/b7e/fc36dcb7e766ff70cea733871fb08318.svg) (в том числе значение на последнем шаге ![$Z_{last\_step}$](https://habrastorage.org/getpro/habr/formulas/6f3/d1e/71f/6f3d1e71f25d0186764308a101007102.svg)), соответствующие нужному уровню значимости, зависят от номера проверки ![$step\_number$](https://habrastorage.org/getpro/habr/formulas/6ad/89b/45e/6ad89b45e341838504aaed7af416b106.svg) (принимает значения от 1 до общего количества проверок ![$total\_steps$](https://habrastorage.org/getpro/habr/formulas/cac/545/111/cac545111861f1a1af134e033ec66c37.svg) включительно) и рассчитываются по эмпирически полученной формуле:  

![$Z_{last\_step} = 2.2471 + \frac {0.3373} {total\_steps} − \frac {0.6331} {\sqrt {total\_steps}} \\ Z_{step} = Z_{last\_step} \ {\sqrt { \frac {total\_steps} {step\_number}}}$](https://habrastorage.org/getpro/habr/formulas/c3d/526/aa6/c3d526aa6e952b8ccae7d2709e8f1428.svg)

  

**Код для воспроизведения коэффициентов**

  
Соответствующие уровни значимости вычисляются через перцентиль ![$perc$](https://habrastorage.org/getpro/habr/formulas/9ba/b15/7ea/9bab157ea4787d148c38d9a008317138.svg) стандартного распределения, соответствующий значению статистики Стьюдента ![$Z$](https://habrastorage.org/getpro/habr/formulas/210/e87/d52/210e87d52a95aed4bdae31d76f553306.svg):  
  

```
perc = scipy.stats.norm.cdf(Z)pval_thresholds = (1 − perc) * 2
```

  
На тех же симуляциях это выглядит так:  
  
![](https://habrastorage.org/r/w1560/webt/yx/f1/gb/yxf1gbwtgaoebxhuzbtodkwychg.png)  
  
Ложноположительных результатов получилось 501 из 10 тысяч, или ожидаемые 5%. Обратите внимание, что уровень значимости не достигает значения в 5% даже в конце, так как эти 5% должны «размазаться» по всем проверкам. В компании мы пользуемся именно этой поправкой, если запускаем тест с возможностью ранней остановки. Прочитать про эти же и другие поправки можно по [ссылке](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5052936/).  
  

**Метод Optimizely**

  

## Калькулятор A/B-тестов

  
Специфика нашего продукта такова, что распределение любой метрики очень сильно меняется в зависимости от аудитории теста (например, номера класса) и времени года. Поэтому не получится принять за дату окончания теста правила в духе «тест закончится, когда в каждой группе наберётся 1 млн пользователей» или «тест закончится, когда количество решённых заданий достигнет 100 млн». То есть получится, но на практике для этого надо будет учесть слишком много факторов:  
  

- какие классы попадают в тест;
- тест раздаётся на учителей или учеников;
- время учебного года;
- тест на всех пользователей или только на новых.

  
Тем не менее, в наших схемах A/B-тестирования всегда нужно заранее фиксировать дату окончания. Для прогноза продолжительности теста мы разработали внутреннее приложение — калькулятор A/B-тестов. Основываясь на активности пользователей из выбранного сегмента за прошлый год, приложение рассчитывает срок, на который надо запустить тест, чтобы значимо зафиксировать аплифт в X% по выбранной метрике. Также автоматически учитывается поправка на множественную проверку и рассчитываются пороговые уровни значимости для досрочной остановки теста.  
  
![](https://habrastorage.org/r/w1560/webt/s9/8z/az/s98zazkhmbfs51k6k_ahhasvbe8.png)  
  
Все метрики у нас рассчитываются на уровне объектов теста. Если метрика — количество решённых задач, то в тесте на уровне учителей это будет сумма решённых задач его учениками. Так как мы пользуемся критерием Стьюдента, можно заранее рассчитать нужные калькулятору агрегаты по всем возможным срезам. Для каждого дня со старта теста нужно знать количество людей в тесте ![$users\_cnt$](https://habrastorage.org/getpro/habr/formulas/2f5/03c/82a/2f503c82a7526312c9d43811b55d9393.svg), среднее значение метрики ![$metric\_mean$](https://habrastorage.org/getpro/habr/formulas/3b9/5f3/166/3b95f316685f025c5df137e1c9c31026.svg) и её дисперсию ![$metric\_std$](https://habrastorage.org/getpro/habr/formulas/523/7e7/b80/5237e7b80ec2e100368ee197c1839554.svg). Зафиксировав доли контрольной группы ![$control\_group\_share$](https://habrastorage.org/getpro/habr/formulas/7af/908/4bd/7af9084bde41fadec87b8adb733c8d74.svg), экспериментальной группы ![$exp\_group\_share$](https://habrastorage.org/getpro/habr/formulas/28a/3a6/370/28a3a6370432fc2bc496e085e3953519.svg) и ожидаемый прирост от теста ![$uplift\_expected$](https://habrastorage.org/getpro/habr/formulas/c8c/2fe/3ba/c8c2fe3ba0e4a3c098c4aa9b97b9a238.svg) в процентах, можно рассчитать ожидаемые значения статистики Стьюдента ![$ttest\_stat\_value$](https://habrastorage.org/getpro/habr/formulas/089/5f8/638/0895f86382784e94886e777cf597eeb9.svg) и соответствующее p-value на каждый день теста:  
  

![$ttest\_stat\_precursor = \frac{metric\_mean\ \sqrt {users\_cnt}}{metric\_std} \\ ttest\_stat\_value = \frac {ttest\_stat\_precursor} {\sqrt{ \frac{1} {control\_group\_share} + \frac {1} {exp\_group\_share}}} * uplift\_expected / 100$](https://habrastorage.org/getpro/habr/formulas/fdd/af7/d20/fddaf7d20ba75af7c3c2e6eee2875d48.svg)

  
Далее легко получить значения p-value на каждый день:  
  

```
pvalue = (1 − scipy.stats.norm.cdf(ttest_stat_value)) * 2
```

  
Зная p-value и уровень значимости с учетом всех поправок на каждый день теста, для любой продолжительности теста можно рассчитать минимальный аплифт, который можно задетектировать (в англоязычной литературе — MDE, minimal detectable effect). После этого легко решить обратную задачу — определить количество дней, необходимое для выявления ожидаемого аплифта.  
  

## Заключение

  
В качестве заключения хочу напомнить основные посылы статьи:  
  

- Если вы сравниваете средние значения метрики в группах, скорее всего, вам подойдёт критерий Стьюдента. Исключение — экстремально малые размеры выборки (десятки наблюдений) или аномальные распределения метрики (на практике я таких не встречал).
- Если в тесте несколько групп, пользуйтесь поправками на множественное тестирование гипотез. Подойдёт простейшая поправка Бонферрони.
- Сравнения по дополнительным метрикам или срезам групп тоже подвержены проблеме множественного тестирования.
- Выбирайте дату завершения теста заранее. Вместо даты также можно зафиксировать количество наблюдений в группе.
- Не подводите итоги теста раньше этой даты. Это можно делать, только если вы заранее решили пользоваться методами, подразумевающими досрочное завершение, например, методом O'Brien-Fleming.
- Когда вносите изменения в схему A/B-тестирования, всегда проверяйте её жизнеспособность A/A-тестами.

  
Несмотря на всё вышенаписанное, бизнес и здравый смысл не должны страдать в угоду математической строгости. Иногда можно выкатить на всех функционал, не показавший значимого прироста в тесте, какие-то изменения неизбежно происходят вообще без тестирования. Но если вы проводите сотни тестов в год, их аккуратный анализ особенно важен. Иначе есть риск, что количество ложноположительных тестов будет сравнимо с реально полезными.