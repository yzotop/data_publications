---
tags:
  - data
author: Lunin
link: https://habr.com/ru/companies/avito/articles/590105/
source: habr
data_type:
  - AB tests
company: Avito
---

Всем привет! Меня зовут Дима Лунин, и я аналитик в Авито. Как и в большинстве компаний, наш основной инструмент для принятия решений — это A/B-тесты. Мы уделяем им большое внимание: проверяем на корректность все используемые критерии, пытаемся сделать результаты более интерпретируемыми, а также увеличиваем мощность критериев. Про это всё мы уже написали две статьи на Хабр, вот [первая часть](https://habr.com/ru/company/avito/blog/571094/), а вот — [вторая](https://habr.com/ru/company/avito/blog/571096/).

В текущем посте я хочу рассказать, как ещё сильнее увеличить мощность критериев для A/B-тестирования, используя машинное обучение. В некоторых моментах буду ссылаться на две предыдущие статьи, так что если вы их ещё не читали, самое время это исправить.

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/11c/30b/569/11c30b569a0888f37e6a1e8d620c9d38.png)

Кратко, о чём я собираюсь рассказать:

1. Что такое CUPED-метод.
    
2. Как улучшить CUPED-алгоритм. CUPAC, CUNOPAC и CUMPED — подробно про каждый из них. 
    
3. Как использовать Uplift-модель в качестве статистического критерия. Здесь я продемонстрирую все прелести bootstrap-технологий.
    
4. Как использовать все модели сразу для достижения лучшей мощности.
    
5. Насколько эти методы вместе [с парной стратификацией](https://habr.com/ru/company/avito/blog/571096/) лучше, чем обычный CUPED.
    

Отдельно отмечу, что такие методы, как CUNOPAC, CUMPED, критерий на основе Uplift-модели и критерий, объединяющий несколько критериев, были разработаны и придуманы нашей командой.

## Задачи для проверки критериев

Прежде чем приступить к рассказу о критериях, хочу показать, для каких задач годятся описанные далее алгоритмы.

**Поюзерные A/B-тесты.** Здесь вы одним пользователям показываете новый дизайн, новые фишки и так далее, а в другой группе оставляете всё как было. Ждёте какой-то срок и смотрите с помощью статистического критерия, прокрашен тест или нет.

Конечно, в таких экспериментах хочется иметь наиболее мощный критерий: так мы сможем проверить больше гипотез за меньшее время. Ещё примеры, зачем может потребоваться большая мощность у критерия, можно найти в статьях выше.

Но бывают случаи, когда такое не получается сделать. Например, вы тестируете новую рекламу на билбордах или телевидении. Тогда вы не можете одним пользователям в Москве показывать новую рекламу, а вторым в этот момент завязать глаза.

То же самое с новыми продуктами. Мы в Авито тестировали в своё время новые услуги продвижения. Если бы мы проводили обычный поюзерный A/B-тест, то в поисковой выдаче были бы два типа объявлений: с новыми услугами продвижения и со старыми. И это нарушило бы чистоту эксперимента: при раскатке у нас все объявления будут с новыми услугами в поисковой выдаче, а результаты A/B мы получили на смешанной поисковой выдаче. Здесь поможет другой тип экспериментов.

**Региональные A/B-тесты.** Давайте перейдём к новой статистической единице — региону. Например, будем показывать нашу рекламу или введём новые услуги только в половине регионов России. Тогда всё честно: рекламу в одном регионе не увидят пользователи из других регионов (а точнее число тех, кто увидит, будет пренебрежимо мало). И выдача объявлений не пересекается по регионам. Так что пользователям Ростовской области можно разрешить купить новые услуги продвижения, а в Краснодарском крае — нет, они друг на друга не влияют.

С точки зрения математики это означает, что вместо гигантского количества пользователей в A/B-тесте у нас будет примерно 85 элементов-регионов: 42 из них — в тесте и 43 — в контроле. В этом случае мы также можем применять статкритерии, которые используются для поюзерных экспериментов. Но элементов всего 85, нормально ли они себя покажут? Не будут ли критерии строить некорректный доверительный интервал?

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/330/4b9/5f3/3304b95f3886ffdbe4d9371ef79b2789.png)

Главный минус таких тестов — они слишком шумные. Чтобы задетектировать хоть какой-то эффект, надо, чтобы он был огромным. Поэтому если будет алгоритм, который сможет очень сильно увеличить мощность критерия, или, что эквивалентно, сократить доверительный интервал для эффекта, это будет мегаполезно для бизнеса.

На этих двух задачах я и буду тестировать все представленные далее критерии и на них же покажу результаты. Теперь предлагаю перейти к сути статьи. Начнём с CUPED-алгоритма.

## CUPED

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/055/f3d/793/055f3d79327eb6dc07bb0f4b7f087620.png)

**CUPED (Controlled-experiment Using Pre-Experiment Data)** — очень популярный в последнее время метод уменьшения вариации. Чтобы понять, как он работает, рассмотрим искусственный пример.

Пусть ваша метрика — выручка, и эксперимент длится месяц. Вы собрали данные во время эксперимента, какая выручка от пользователей в тесте и в контроле:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/861/ae4/e77/861ae4e775d4e53f7e86eae037f1d797.png)

В итоге метрики очень шумные: в тесте даже на трёх точках видно, что значения разнятся от 50 до 150, а в контроле — от 20 до 200 рублей. С помощью T-test получились следующие результаты: **+10±20 ₽** на одного пользователя, результат не статзначим. Как это исправить?

Давайте добавим информацию с предпериода: сколько эти пользователи тратили в среднем в месяц **до начала эксперимента.** При правильном сетапе теста матожидание этой величины одинаково в тесте и в контроле. Тогда:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/e1d/8f2/b67/e1d8f2b67fc625cf4f4bc622d7a823cc.png)

Будем теперь смотреть на дельту. Это разница выручки на экспериментальном периоде минус средняя выручка на предпериоде в месяц.

Во-первых, почему так можно? В тесте и в контроле мы вычитаем с точки зрения матожидания одно и то же, потому что в правильно проведённом A/B-тесте контроль и тест ничем не отличаются на предпериоде. Поэтому, когда мы смотрим на разницу, эти вычитаемые величины нивелируются.

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/752/a1f/ca6/752a1fca6395b607dc46cf941b099c02.png)

Здесь:

- T — экспериментальная метрика в тесте.
    
- C — экспериментальная метрика в контроле.
    

Во-вторых, зачем нам всё это?

Посмотрим на разброс значений в delta: в контроле от -10 до 6 рублей, а в тесте от -10 до 20 рублей. Это намного меньше того разброса, что мы видели ранее. Поэтому и доверительный интервал сократится: результат станет **+15±5 ₽** для одного пользователя, и он уже статзначим! Почему дисперсия уменьшилась? Потому что мы смогли объяснить часть данных в эксперименте с помощью предпериода. 

А теперь посмотрим на CUPED более формально. 

Основная идея CUPED-метода: давайте вычтем что-то из теста и из контроля так, чтобы матожидание разницы новых величин осталось таким же, как и было. Но дисперсия при этом уменьшилась бы:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/175/76b/1a0/17576b1a0eb3b895c1f9e568b6e03fa6.png)

Где A и B — некоторые случайные величины (ковариаты). Тогда утверждается, что если θ будет такой, как указано в формулах далее, то дисперсия будет минимально возможной для таких статистик.

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/8f2/cf9/f03/8f2cf9f036b1646c38cafb4f02514c76.png)

В примере выше θ я взял равной 1, но на самом деле, лучше было подобрать значение по этой формуле. Тогда формула для дисперсии получается такой:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/cca/08e/e48/cca08ee48a6fb5952d6acc1e963016e7.png)

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/efd/f8d/da0/efdf8dda0c4e01d07f7e772a77190b38.png)

Поэтому, чем больше корреляция по модулю, тем меньше будет дисперсия.

**Также важно помнить**: чтобы метод работал корректно, достаточно, чтобы матожидания A и B совпадали:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/237/7de/c25/2377dec257c0b7032074bed7fcd8029c.png)

Ещё больше интересных и нетривиальных моментов про CUPED расписано [в прошлой статье](https://habr.com/ru/company/avito/blog/571096/). В частности, там есть информация о том, как построить относительный CUPED-критерий.

Осталось понять, что брать в роли A и B. Чаще всего для них берут значения той же метрики, но на предэкспериментальном периоде. В примере выше мы брали среднюю выручку за месяц. Чем хорош такой способ:

1. Матожидание метрики на предпериоде будет одним и тем же в тесте и в контроле — иначе у вас некорректно поставлен A/B-тест. А значит, CUPED даст правильный результат.
    
2. В большинстве случаев метрика на предпериоде сильно коррелирует с экспериментальным периодом. Отсюда получается, что и дисперсия сильно уменьшится.
    

Но кроме значения метрики на предпериоде можно использовать результаты ML-модели, обученной предсказывать истинные значения метрик без влияния тритмента. С хорошей моделью можно достичь большего уменьшения дисперсии. Давайте поговорим о том, как это сделать.

## CUPAC

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/474/b4e/f95/474b4ef9545399e0a2e6f8998db06528.png)

**CUPAC (Controlled-experiment Using Prediction As Covariate)** — первая вариация на тему CUPED с ML. Как я уже писал выше, давайте попробуем использовать предсказание в качестве ковариаты. Распишу сам предлагаемый алгоритм на примере определённой задачи. Для ваших задач вы сможете переиспользовать алгоритм по аналогии.

Пусть у нас есть A/B-тест, где:

- Дата начала — 1 июня.
    
- Длительность теста составляет 1 месяц.
    
- Наша ключевая метрика — выручка.
    
- 1 месяц уже прошёл.
    

Чтобы оценить такой тест, мы проделаем следующую операцию **из трёх основных шагов**:

1. Соберём датасет для предсказания (или «экспериментальный датасет») и обучения.
    
2. На обучающем датасете обучим нашу модель, подберём гиперпараметры, а на экспериментальном будем предсказывать таргет, используя фичи с предпериода.
    
3. Мы получили новую ковариату и запускаем обычный CUPED-алгоритм.
    

На схеме это можно изобразить так:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/5d0/10f/da8/5d010fda8b4cfe91f5fc2a5262cd99bc.png)

Как собрать экспериментальный и обучающий датасеты?

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/c4d/474/064/c4d4740643dbb8f4f3aac2fd48ce8bea.png)

На иллюстрации сине-красный датасет — экспериментальный, а бирюзовый — обучающий, собранный до 1 июня. Одним цветом отмечено, что пользователи друг от друга в среднем ничем не отличаются, разными цветами — между ними есть разница.

Теперь опишем сам алгоритм в тексте. Повторим в цикле, пока не жалко:

1. Вычитаем из текущей даты длительность эксперимента. Начинаем с 1 июня, потом получим дату 1 мая, потом — 1 апреля и так далее.
    
2. Теперь собираем таргет. Для каждого юзера сохраняем его суммарные траты у нас на сайте за этот месяц. Для 1 июня это экспериментальная выручка за июнь (и уже за июнь вся выручка известна), для мая это выручка за май, когда ещё не было никаких отличий между тестом и контролем, и так далее.
    
    - Тут у нас получатся один сине-красный датасет с экспериментальной выручкой (и она отлична у теста с контролем) и много бирюзовых датасетов, где пользователи в среднем неразличимы.
        
3. И теперь собираем датасет фичей. Главная особенность состоит в том, что фичи должны быть собраны до текущей рассматриваемой даты. То есть для 1 июня все фичи должны быть собраны по данным **до 1 июня,** аналогично для остальных дат.
    
    - Например, количество заходов на сайт за N месяцев до рассматриваемой даты (за 2 месяца до 1 июня; за 2 месяца до 1 мая и так далее).
        
    - Выручка от пользователя за разные промежутки времени (за 1 месяц, за 2 месяца и так далее).
        
    - Сколько месяцев назад зарегистрировался пользователь.
        
4. **Ни в одном из датасетов нет фичи тестовая или контрольная группа. Важно, чтобы пользователи по обучающим фичам в тесте и в контроле были полностью идентичны.**
    

Что важно проверить: нужно, чтобы матожидания новых ковариат совпадали. А для этого достаточно, чтобы признаки ML-модели в среднем не отличались в тесте и в контроле. Далее идёт теоретическое пояснение этому факту.

Теоретическое пояснение

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/7fa/389/70d/7fa38970d18abdc1093ca0ea893f3eaa.png)

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/ecf/0e1/ead/ecf0e1eadbf8d3643b3da0ae25b426e5.png)

Вот и всё! Это полный пайплайн работы CUPAC-алгоритма. Теперь я попытаюсь объяснить, зачем нужен тот или иной шаг.

Вопросы и ответы к методу

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/537/208/4cb/5372084cbc5e3847a357b466e6eb53c9.png)

**Про результаты работы критерия**: как и ранее, я буду сравнивать алгоритмы по метрике «ширина доверительного интервала». Чем она меньше, тем лучше.

- На поюзерных тестах: уменьшение доверительного интервала на 1000 A/A-тестах в среднем на 4% относительно CUPED.
    
- На региональных тестах: уменьшение доверительного интервала на 1000 A/A-тестах  в среднем на 22% относительно CUPED.
    

**Если вы решите реализовать этот или любой другой алгоритм из статьи, то обязательно проверьте его** [**корректность**](https://habr.com/ru/company/avito/blog/571094/)**!**

Это всё, что здесь можно рассказать. Теперь перейдём к «нечестному» CUPAC и избавимся от большинства шагов в алгоритме.

## CUNOPAC

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/475/fba/e45/475fbae459456b2dd7c2da6f55efc7a7.png)

**CUNOPAC (Controlled-experiment Using Not Overfitted Prediction As Covariate)** — чуть более простая, чуть менее корректная и чуть более мощная модель, чем CUPAC. В вопросах к CUPAC я писал, что обучающий датасет нам нужен, ведь иначе может возникнуть проблема переобучения. Но я предлагаю забыть про эту проблему!

Главное, о чём надо помнить в таком случае — **вы** **можете построить некорректный критерий!** Он будет строить зауженный доверительный интервал, и вместо 95% случаев будет покрывать лишь 85%. Но об этом я расскажу далее.

**Алгоритм работы критерия такой:**

1. Соберём экспериментальный датасет для предсказания.
    
2. На нём обучим нашу модель, подберём гиперпараметры, и на нём будем предсказывать таргет, используя фичи с предпериода. Обучаемся на всем датасете сразу! И предсказываем весь датасет. Ничего не откладываем на валидацию (да, метод рискованный).
    
3. Мы получили новую ковариату и запускаем обычный CUPED-алгоритм.
    

На схеме это можно изобразить так:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/fa7/41d/32d/fa741d32d900d2e1cc60c20eb21cf78a.png)

Датасет собираем также, как и в CUPAC-алгоритме, только обучающий датасет больше не нужен:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/fac/eb1/c26/faceb1c2609d8f7bf3856a7658420146.png)

**Теперь вопрос: как подобрать модель, которая не переобучится?** Мы решили этот вопрос так: отбираем небольшое количество фичей и используем только линейные модели.

Рассмотрим пример: что, если взять CatBoost-алгоритм или линейную регрессию в качестве ML-алгоритма внутри критерия? Посмотрим на реальный уровень значимости (или False Positive Rate), полученный при проверке метода на 1000 А/А-тестах:

- Линейные модели: 0.055 (при alpha=0.05), результат статзначимо не отличается от теоретического alpha.
    
- CatBoost: **0.14** (при alpha=0.05), результат статзначимо **отличается** от теоретического alpha!
    

То есть CatBoost ошибается примерно в 3 раза чаще, чем должен был! Он переобучается, заужает доверительный интервал и поэтому критерий некорректен. Что это значит на практике? Что вы в 3 раза чаще будете катить изменение, которое на самом деле не имеет никакого эффекта. Поэтому CatBoost нельзя использовать внутри CUNOPAC, а линейные модели можно. При этом я не говорю, что CatBoost плохой алгоритм! Он отличный, просто он легче переобучается.

Есть ли переобучение у линейной модели? Наверняка есть, но оно настолько мало, что не влияет на эффект.

А почему качество CUNOPAC лучше, чем у CUPAC?

Что ещё хочется отметить: это очень опасная модель! Если модель переобучиться, вы будете заужать доверительный интервал и ошибка будет больше заявленной. **Поэтому снова предупреждаю: обязательно проверьте ваш** [**критерий**](https://habr.com/ru/company/avito/blog/571094/)**!**  

**Результаты работы критерия:**

- На поюзерных тестах: уменьшение доверительного интервала на 9% относительно CUPED.
    
- На региональных тестах: не удалось использовать, реальный уровень значимости сильно больше тестируемых 5%, слишком мало регионов. Модель просто всегда переобучается.
    

Проверено на 1000 A/А-тестах.

А теперь обсудим, как построить пайплайн обучения модели для этих двух критериев.

## Автоматизация ML-части критериев

Поговорим про основную часть алгоритмов. Что у нас есть: обучающий и экспериментальный датасеты, которые мы будем скармливать модели. В случае CUNOPAC это один и тот же датасет. Расскажу, как я воплотил ML-часть алгоритмов, но это не значит, что у меня получилась идеальная автоматизация: возможно, вы сможете придумать лучше. При этом, если вы плохо понимаете идеи и алгоритмы машинного обучения, то можете пропустить эту часть. Она никак не повлияет на осознание всех алгоритмов далее.

Краткая иллюстрация:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/1f2/807/a7f/1f2807a7f3ac7857aa06764287dab9db.png)

**Шаг 1.** Выбираем класс моделей, который будем тестировать. Например, я использовал:

Для CUPAC: `Lasso, Ridge, ARDRegression, CatBoostRegressor, RandomForestRegressor, SGD, XGBRegressor.`

Для CUNOPAC: `Lasso, ARDRegression, Ridge.`

Дополнительно: для адекватности я нормирую все признаки с помощью StandardScaler.

**Шаг 2.** Выбираем фичи для обучения. Ограничим сверху количество используемых фичей и воспользуемся одним из алгоритмов подбора признаков:

1. Жадный алгоритм добавления/удаления признаков (например, SequentialFeatureSelector из библиотеки scikit-learn на Python).
    
2. Алгоритм на основе feature_importance параметра фичей у модели (SelectFromModel из библиотеки scikit-learn на Python). Самый быстрый из алгоритмов.
    
3. Add-del алгоритм (добавляем фичи, пока качество улучшается, потом выкидываем фичи, пока качество улучшается, повторим процедуру снова, пока не сойдёмся).
    
4. Всё, что ещё найдёте или придумаете.
    

Единственное замечание: если у алгоритма отбора фичей можно задать функцию оценки для качества модели, то стоит использовать специальную функцию-скоррер:

```
def cuped_std_loss_func(y_true, y_pred):    theta = np.cov(y_true, y_pred)[0, 1] / (np.var(y_pred) + 1e-5)    cuped_metric = y_true - theta *  y_pred    return np.std(cuped_metric)
```

Так вы будете оценивать ровно ту величину, которую вы хотите оптимизировать — ширину доверительного интервала у CUPED-метрики.

**Шаг 3.** Следующая остановка — выбор гиперпараметров. Мы считаем в этот момент, что все обучающие признаки уже выбраны. Есть следующие варианты:

1. Перебор по GridSearchCV.
    
2. Перебор по RandomizedSearchCV.
    
3. Жадный перебор по одному параметру:
    
    - сначала подбираем по функции cuped_std_loss_func первый в списке гиперпараметр;
        
    - потом подбираем второй в списке, при учёте выбранного первого гиперпараметра и т.д. Пример с кодом:
        

```
start_params = {    'random_state': [0, 2, 5, 7, 229, 13],    'loss': ['squared_loss', 'huber'],    'alpha': np.logspace(-4, 2, 200),    'l1_ratio': np.linspace(0, 1, 20),} # Пример перебора параметров для SGDdef create_best_model_with_grid_search(pipeline, start_params, X, y):        final_params = {}        params = []        for param in start_params.items():            params_dict = {}            params_dict[param[0]] = param[1]            params.append(params_dict)                 score = make_scorer(cuped_std_loss_func, greater_is_better=False)        for param in params:            search_model = GridSearchCV(pipeline, param, cv = 2, n_jobs = -1,                                        scoring = score)            search_model.fit(X, y)            final_params = {**final_params, **search_model.best_params_}            pipeline.set_params(**final_params)        return pipeline, final_params
```

Плюс такого метода: скорость по сравнению с первым методом. Вместо всех возможных комбинаций здесь перебирается линейное количество вариантов. Все значения гиперпараметров будут рассмотрены по сравнению со вторым методом.

Минус: GridSearchCV даст лучшее качество, но работать будет сильно дольше.

Ещё, конечно, можно объединить предыдущий шаг с этим: для каждого набора фичей подобрать гиперпараметры и измерять качество по cuped_std_loss_func. Но это работает очень долго. Можно также устроить сходящуюся процедуру из этих двух шагов: выбираем признаки, выбираем гиперпараметры, снова выбираем признаки с подобранными значениями гиперпараметра и так далее.

**Шаг 4.** Отлично, после обучения у нас есть N моделей с первого шага с подобранными гиперпараметрами и обучающими признаками для каждой из них. Отберём из них одну модель, которая даёт наилучшее качество на экспериментальном датасете по cuped_std_loss_func. Почему так можно сделать, я расскажу, когда буду описывать Frankenstein-критерий. 

А пока перейдём к третьему критерию, который, грубо говоря, не является ML-критерием.

## CUMPED

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/f26/8f5/8ec/f268f58ec7bb804f1ca6d5ee83a3bf60.png)

Почему нам вообще нужна только одна ковариата? Почему нельзя использовать сразу N ковариат? Именно на этой идее построен алгоритм **CUMPED** **(Controlled-experiment Using Multiple Pre-Experiment Data).**

Ранее в CUPED было так:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/267/850/f34/267850f34587811b2fc1353b1a49abc5.png)

А теперь в CUMPED будет так:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/b30/041/bb9/b30041bb941733f200db9d6ca11d5312.png)

При этом не обязательно использовать только две ковариаты, их может быть сколько угодно!

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/853/d12/6d4/853d126d41fc9b3450a8c16aa874da55.png)

Зачем нам вообще может потребоваться N ковариат? Почему одной недостаточно?

Как я писал в CUPED-алгоритме, из-за того, что часть метрик T и C описывается данными из ковараиты, мы и сокращаем дисперсию. А если признаков будет не один, а сразу несколько, то мы лучше опишем нашу метрику T и C, а значит сильнее сократим дисперсию. Это то же самое, что и в машинном обучении: что лучше, использовать один признак для обучения или несколько различных фичей? Обычно выбирают второе.

Теперь опишем **первую** версию предлагаемого алгоритма CUMPED:

1. Соберём один экспериментальный датасет, как сделали это в CUNOPAC.
    
2. В цикле по всем признакам, которые использовались ранее для предсказания метрики:
    
    - Образуем CUPED-метрику T', С', используя в качестве ковариат текущие признаки в цикле.
        
    - В следующий раз в качестве изначальных метрик будут использоваться новые CUPED-метрики. То есть на следующей итерации цикла вместо текущих метрик T и C будут использоваться полученные сейчас T', C'.
        

Если расписать формулами, то:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/be9/719/e6f/be9719e6f10b19a4c4ea95b7683eb720.png)

Где наша итоговая CUMPED-метрика это T^N, C^N, а A_i, B_i — это признаки пользователя, собранные на предпериоде. Раньше мы на них обучали модель, а сейчас используем в качестве ковариат. Коэффициенты тета подбираются следующим образом:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/0dc/572/929/0dc5729295d15c0915819c10f5ae504f.png)

Вот и весь алгоритм. Но! Если бы вы его реализовали, то он, вероятно, показал бы себя хуже, чем обычный CUPED-алгоритм. Почему?

В этом алгоритме есть одна беда:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/c36/e1d/b01/c36e1db015760178b953b7754f2fcc6a.png)

**Порядок применения ковариат важен!** Если сначала вычесть ковариату A_1, а потом ковариату А_2, то дисперсия может быть больше, нежели в случае, когда вы сначала вычтете A_2, а потом A_1.  Вы можете сами расписать формулы, и увидеть, что две эти случайные величины будут иметь разную дисперсию.

**Поэтому предлагается применять ковариаты в «жадном» порядке**: сначала ищем наилучшую ковариату, уменьшающую дисперсию сильнее всего, потом следующую ковариату, уменьшающую дисперсию лучше всего для T^1, C^1  и т. д. То есть в первой версии алгоритма мы берём не случайную ковариату, а ту, которая на текущий момент сильнее всего уменьшает доверительный интервал.

На рисунке представлен **итоговый** алгоритм CUMPED:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/325/209/9e7/3252099e79df40012b3f3a1d5ce80ac1.png)

Иллюстрация подбора наилучшей ковариаты:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/3f4/8c9/002/3f48c9002ee2855801154f5da8a32c54.png)

Вопросы и ответы к методу

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/cd5/970/c0d/cd5970c0d23953b098bf06794c367488.png)

```

```

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/03d/645/017/03d6450175fffc9003627419d012f0e1.png)

**Результаты работы критерия:**

- На поюзерных тестах: уменьшение доверительного интервала **на 14%** относительно CUPED. Это наилучший результат среди всех представленных здесь алгоритмов.
    
- На региональных тестах: уменьшение доверительного интервала на 21% относительно CUPED. Но можно использовать лишь одну ковариату! Почему так, я ответил в скрытом разделе с вопросами.
    

Проверено на 1000 A/А-тестах.

## Uplift-критерий

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/605/c14/84b/605c1484b3e70b1fe668665ef1d61b9e.png)

Всё, забыли про CUPED и всё, что с ним связано. В этом разделе я покажу, как из Uplift-модели сделать настоящий критерий. Но для начала о том, что такое Uplift-модель.

Давайте соберём датасет так же, как ранее это делали в CUNOPAC и в CUMPED, но в этот раз добавим фичу: какая группа пользователя. 1 — пользователь в тесте, 0 — в контроле.

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/099/2c8/6bd/0992c86bd7e59a7ebcd83091c23e668a.png)

Теперь посмотрим, как работает алгоритм:

1. Разделим датасет на K частей.
    
2. Будем в цикле размера K выбирать часть пользователей, которым будем предсказывать их метрику. На оставшейся части обучим ML-алгоритм. Эти два шага в ML зовутся кросс-валидацией.
    
3. Теперь для каждого пользователя на тестовой части предскажем его выручку, если бы он был в тесте и если бы он был в контроле. Как мы это сделаем? Поменяем в датасете фичу группы пользователя: на 1, если предсказываем выручку от него в тесте, и на 0, если предсказываем его выручку в контроле. Обозначим эти значения T_p и C_p. Теперь оценка Uplift для пользователя — это предсказанная выручка, если пользователь в тесте минус предсказанная выручка, если пользователь в контроле.
    
4. Повторим K раз и получим Uplift для всех пользователей.
    

На рисунке я кратко описал то же самое. Возможно, так будет понятней:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/8b1/997/3ad/8b19973adc029ebc3f51ae8aa8106a9e.png)

Мы смогли получить численную оценку эффекта для каждого пользователя. Однако есть несколько «но»:

1. Как из оценки построить доверительный интервал?
    
2. А если модель смещённая или некорректная? Например, она никак не учитывает группу пользователя внутри себя: тогда Uplift будет равен 0, что может быть не так. А если для тестовых юзеров он постоянно занижает предсказание, а для контрольных юзеров завышает?
    

Чтобы решить обе проблемы, введём ошибку предсказания:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/88e/269/8cf/88e2698cf5adef63703ca82824d0a4f1.png)

Тогда:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/f8c/ae0/b16/f8cae0b16b208a8524757905fff9dadf.png)

То есть наш Uplift будет равен разнице предсказаний + разнице ошибок предсказаний в тесте и в контроле. Отличие по сравнению со старой формулой в том, что теперь мы добавляем ошибку предсказания: если мы занижаем предсказание на тестовых юзерах, то ошибка предсказания на тесте будет положительной. То же самое для контрольных юзеров.

Тогда численную оценку Uplift можно представить следующим образом:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/2f8/d0b/cf3/2f8d0bcf379ed7ad7554154c743cf908.png)

Но и здесь есть проблема: размер выборок у трёх слагаемых разный! Давайте посмотрим, а что мы вообще можем посчитать для каждого пользователя.

Для пользователя в тесте:

- Значение метрики в тесте (или T в формулах выше).
    
- Предсказание модели, если пользователь в тесте (или T_p в формулах выше).
    
- Предсказание модели, если пользователь в контроле (или C_p в формулах выше).
    
- Ошибку предсказания модели, если пользователь в тесте (или ε_T в формулах выше).
    

Для пользователя в контроле:

- Значение метрики в контроле (или C в формулах выше).
    
- Предсказание модели, если пользователь в тесте (или T_p в формулах выше).
    
- Предсказание модели, если пользователь в контроле (или C_p в формулах выше).
    
- Ошибку предсказания модели, если пользователь в контроле (или ε_C в формулах выше).
    

Поэтому, размер выборок на самом деле будет таким:

- T_p, С_p: их размер равен количеству пользователей в тесте и в контроле, или |T| + |C| .
    
- ε_C: размер этой выборки равен количеству пользователей в контроле, или |C|.
    
- ε_T: размер выборки равен количеству пользователей в тесте, или |T|.
    

Вкратце это выглядит так:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/36e/d21/676/36ed216767e740e456ea69066396e45c.png)

Выпишем формулу Uplift с учётом всех вводных:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/274/ed7/44d/274ed744d2876325d5d138209a7db549.png)

Так, а теперь вопрос. **Как для такой статистики построить доверительный интервал?** Ведь T_p, C_p, ε_T, ε_C ещё и коррелируют между собой? Как я упоминал в предыдущих статьях, если не знаете, как что-то посчитать теоретически, то используйте **bootstrap.** В данном случае достаточно забутстрапить каждого пользователя в тесте и в контроле по отдельности, а точнее их метрики T_p, С_p, ε_C, ε_T. Далее на каждой итерации внутри бутстрапа посчитаем оценку Uplift для них по формуле выше, и по этим данным построим доверительный интервал.

Замечание

**Результаты работы критерия:**

- На поюзерных тестах: уменьшение доверительного интервала на 4**%** относительно CUPED.
    
- На региональных тестах: уменьшение доверительного интервала **на 40%** относительно CUPED. Это наилучший результат — обыгрывает CUPAC, CUMPED в 2 раза.
    

Почему произошло столь сильное сокращение на региональных тестах? Потому что на самом деле мы увеличили датасет в 2 раза. Если ранее размер выборки был примерно 42 на 43 (если регионов 85), то сейчас, так как модель предсказывает значения в тесте и в контроле для каждого региона, размер датасета стал 85 на 85. Есть, конечно, оговорка, что мы так увеличили только T_p и C_p, но как видим, это отлично работает.

Проверено на 1000 A/А-тестах.

### Frankenstein-критерий

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/ede/189/16e/ede18916e89eb0b3db0dca7d4b3dba3c.png)

Вот у нас есть четыре критерия, а когда какой стоит использовать? Ранее я писал, что CUMPED лучше всего работает для поюзерных тестов, а Uplift — для региональных. ~~А зачем я рассказывал про все остальные?~~ Но ведь они лучше всего работают в среднем, а не всегда. На самом деле когда-то лучше может сработать CUPED, а когда-то — CUPAC или CUNOPAC. И как в каждом A/B-тесте выбрать, какой критерий сейчас лучше всего использовать?

Для ответа на этот вопрос был придуман критерий-Франкенштейн. Посмотрим на следующую схему:

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/76e/836/626/76e8366267ea08f44babd804e8c422a0.png)

Суть метода проста: берём N критериев и из них выбираем тот, который на текущий момент даёт наименьший доверительный интервал.

Какие критерии использовать внутри? У нас получилась следующая картина.

Для поюзерных тестов стоит использовать:

- CUNOPAC.
    
- CUMPED.
    
- CUPED.
    

CUPAC и Uplift-критерий не стоит использовать, так как они всегда работают хуже, чем CUNOPAC и CUMPED.

А для региональных тестов стоит использовать:

- Uplift-критерий.
    
- CUPAC.
    
- CUMPED.
    
- CUPED.
    

Почему критерий валиден?

[](https://ru.wikipedia.org/wiki/%D0%9F%D0%BE%D0%BF%D1%80%D0%B0%D0%B2%D0%BA%D0%B0_%D0%BD%D0%B0_%D0%BC%D0%BD%D0%BE%D0%B6%D0%B5%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%83%D1%8E_%D0%BF%D1%80%D0%BE%D0%B2%D0%B5%D1%80%D0%BA%D1%83_%D0%B3%D0%B8%D0%BF%D0%BE%D1%82%D0%B5%D0%B7)

- [](https://statisticaloddsandends.wordpress.com/2019/08/25/proof-that-sample-mean-is-independent-of-sample-variance-under-normality/)
    
- [](https://digitalcommons.wayne.edu/cgi/viewcontent.cgi?article=1454&context=jmasm)
    

```

```

![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/bc9/902/363/bc9902363efa5393e8f8a584fabf6800.png)

## Результаты

Поюзерные тесты:

- Сокращение доверительного интервала на 14% относительно CUPED.
    
- С использованием [парной стратификации](https://habr.com/ru/company/avito/blog/571096/): **на 21%.** 
    

Региональные тесты:

- Сокращение доверительного интервала **в 1,5 раза** относительно CUPED.
    
- С использованием [парной стратификации](https://habr.com/ru/company/avito/blog/571096/): **в 2 раза .**
    

Получились отличные результаты! Особенно для региональных тестов.

  
Итого, в данной статье я постарался рассказать про такие методы, как:

- CUPED.
    
- CUPAC, CUNOPAC, CUNOPAC — три вариации на тему CUPED.
    
- Uplift-критерий — как из Uplift-модели сделать критерий.
    
- Frankenstein-критерий — как из нескольких критериев сделать один.
    

Если у вас остались вопросы, обязательно пишите. Постараюсь ответить. Также мне можно писать в соц. сетях:

- тг: @dimon2016
    
- [linkedin](https://www.linkedin.com/in/dimon2016/)