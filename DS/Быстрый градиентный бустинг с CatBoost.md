---
tags:
  - data
source: habr
link: https://habr.com/ru/companies/otus/articles/527554/
data_type:
  - DS
---

**Привет, хабровчане! Подготовили перевод статьи для будущих учеников базового курса [Machine Learning](https://otus.pw/rHFL/).**  
  
![](https://habrastorage.org/r/w1560/webt/yz/l7/su/yzl7suvpy4vatkpztrqe9g1ux5g.png)  
  

---

  
В градиентном бустинге прогнозы делаются на основе ансамбля слабых обучающих алгоритмов. В отличие от случайного леса, который создает дерево решений для каждой выборки, в градиентном бустинге деревья создаются последовательно. Предыдущие деревья в модели не изменяются. Результаты предыдущего дерева используются для улучшения последующего. В этой статье мы подробнее познакомимся с библиотекой градиентного бустинга под названием CatBoost.  
  
![](https://habrastorage.org/r/w1560/webt/l0/4i/ov/l04iovbmunjefjlvo5il9gytpmg.png)  
_[Источник](https://catboost.ai/news/catboost-enables-fast-gradient-boosting-on-decision-trees-using-gpus)_  
  
[CatBoost](https://github.com/catboost) — это библиотека градиентного бустинга, созданная [Яндексом](https://yandex.com/company/). Она использует небрежные (oblivious) деревья решений, чтобы вырастить сбалансированное дерево. Одни и те же функции используются для создания левых и правых разделений (split) на каждом уровне дерева.  
  
![](https://habrastorage.org/r/w1560/webt/kq/km/g0/kqkmg0wv7fehzmkfckw3akrf5d4.png)  
_[Источник](https://catboost.ai/news/catboost-enables-fast-gradient-boosting-on-decision-trees-using-gpus)_  
  
По сравнению с классическими деревьями, небрежные деревья более эффективны при реализации на процессоре и просты в обучении.  
  

## Работа с категориальными признаками

  
Наиболее распространенными способами обработки категориальных данных в машинном обучении является one-hot кодирование и кодирование лейблов. CatBoost позволяет использовать категориальные признаки без необходимости их предварительно обрабатывать.  
  
При использовании CatBoost мы не должны пользоваться one-hot кодированием, поскольку это влияет на скорость обучения и на качество прогнозов. Вместо этого мы просто задаем категориальные признаки с помощью параметра _cat_features._  
  

## Преимущества использования CatBoost

  
Есть несколько причин подумать об использовании CatBoost:  
  

- CatBoost позволяет проводить обучение на нескольких GPU.
- Библиотека позволяет получить отличные результаты с параметрами по умолчанию, что сокращает время, необходимое для настройки гиперпараметров.
- Обеспечивает повышенную точность за счет уменьшения переобучения.
- Возможность быстрого предсказания с применением модели CatBoost;
- Обученные модели CatBoost можно экспортировать в Core ML для вывода на устройстве (iOS).
- Умеет под капотом обрабатывать пропущенные значения.
- Может использоваться для регрессионных и классификационных задач.

  

## Параметры обучения

  
Давайте рассмотрим общие параметры в CatBoost:  
  

- `loss_function` или `objective` – показатель, используемый для обучения. Есть регрессионные показатели, такие как среднеквадратичная ошибка для регрессии и `logloss` для классификации.
- `eval_metric` – метрика, используемая для обнаружения переобучения.
- `Iterations` – максимальное количество построенных деревьев, по умолчанию 1000. Альтернативные названия `num_boost_round`, `n_estimators` и `num_trees`.
- `learning_rate` или `eta` – скорость обучения, которая определяет насколько быстро или медленно модель будет учиться. Значение по умолчанию обычно равно 0.03.
- `random_seed` или `random_state`– случайное зерно, используемое для обучения.
- `l2_leaf_reg` или `reg_lambda` – коэффициент при члене регуляризации L2 функции потерь. Значение по умолчанию – 3.0.
- `bootstrap_type` – определяет метод сэмплинга весов объектов, например это может быть Байес, Бернулли, многомерная случайная величина или Пуассон.
- `depth` = глубина дерева.
- `grow_policy` – определяет, как будет применяться жадный алгоритм поиска. Может стоять в значении `SymmetricTree`, `Depthwise` или `Lossguide`. По умолчанию `SymmetricTree`. В `SymmetricTree` дерево строится уровень за уровнем, пока не достигнет необходимой глубины. На каждом шаге листья с предыдущего дерева разделяются с тем же условием. При выборе параметра `Depthwise` дерево строится шаг за шагом, пока не достигнет необходимой глубины. Листья разделяются с использованием условия, которое приводит к лучшему уменьшению потерь. В `Lossguide` дерево строится по листьям до тех пор, пока не будет достигнуто заданное количество листьев. На каждом шаге разделяется нетерминальный лист с лучшим уменьшением потерь.
- `min_data_in_leaf` или `min_child_samples` – это минимальное количество обучающих сэмплов в листе. Этот параметр используется только с политиками роста `Lossguide` и `Depthwise`.  
    
- `max_leaves` или `num_leaves` – этот параметр используется только с политикой `Lossguide` и определяет количество листьев в дереве.  
    
- `ignored_features` — указывает на признаки, которые нужно игнорировать в процессе обучения.
- `nan_mode` – метод работы с пропущенными значениями. Параметры `Forbidden, Min` и `Max`. При использовании `Forbidden` наличие пропущенных значений вызовет ошибку. При использовании параметра `Min` пропущенные значения будут приняты за максимальные значения для данного признака. В `Max` пропущенные значения будут приняты как минимальные значения для данного признака.  
    
- `leaf_estimation_backtracking`– тип бэктрекинга, использующийся при градиентном спуске. По умолчанию используется `AnyImprovement`. `AnyImprovement` уменьшает шаг спуска до того, как значение функции потерь будет меньшим, чем оно было на последней итерации. `Armijo` уменьшает шаг спуска до тех пор, пока не будет выполнено [условие Вольфе](https://en.wikipedia.org/wiki/Wolfe_conditions#Armijo_rule_and_curvature).
- `boosting_type` — схема бустинга. Она может быть простой для классической схемы градиентного бустинга или упорядоченной, что обеспечит лучшее качество на небольших наборах данных.
- `score_function` – тип [оценки](https://catboost.ai/docs/concepts/algorithm-score-functions.html), используемой для выбора следующего разбиения при построении дерева. `Cosine` используется по умолчанию. Другие доступные варианты `L2, NewtonL2` и `NewtonCosine`.  
    
- `early_stopping_rounds`— если стоит `True`, устанавливает тип детектора переобучения в `Iter` и останавливает обучение, когда достигается оптимальное значение.
- `classes_count` – количество классов для задач мультиклассификации.  
    
- `task_type` – используете вы CPU или GPU. По умолчанию стоит CPU.  
    
- `devices` — идентификаторы устройств GPU, которые будут использоваться для обучения.
- `cat_features` — массив с категориальными столбцами.
- `text_features` —используется для объявления текстовых столбцов в задачах классификации.

  

## Пример с регрессией

  
CatBoost в своей реализации использует стандарт scikit-learn. Давайте посмотрим, как мы можем использовать его для регрессии.  
  
Первый шаг, как всегда, импортировать регрессор и создать его экземпляр.  
  

```
from catboost import CatBoostRegressorcat = CatBoostRegressor()
```

  
При обучении модели CatBoost также позволяет нам визуализировать его, установив _plot=true_:  
  

```
cat.fit(X_train,y_train,verbose=False, plot=True)
```

  
![](https://habrastorage.org/r/w1560/webt/v_/xn/jq/v_xnjqamikbgmx03fsxv2ygh94g.png)  
  
Также мы можем выполнять кроссвалидацию и визуализировать процесс:  
  

```
from catboost import Pool, cvparams = {"iterations": 100,          "depth": 2,          "loss_function": "RMSE",          "verbose": False}cv_dataset = Pool(data=X_train,                  label=y_train)scores = cv(cv_dataset,            params,            fold_count=2,             plot="True")
```

  
![](https://habrastorage.org/r/w1560/webt/xj/u9/2s/xju92swgn2ugokaosuqeizevjx4.png)  
  
Аналогично вы можете выполнить grid search и визуализировать его:  
  

```
grid = {'learning_rate': [0.03, 0.1],        'depth': [4, 6, 10],        'l2_leaf_reg': [1, 3, 5, 7, 9]}grid_search_result = cat.grid_search(grid, X=X_train, y=y_train, plot=True)
```

  
![](https://habrastorage.org/r/w1560/webt/pz/ks/it/pzksitxmprtkao_xljqejpyrx9q.png)  
  
Также мы можем использовать CatBoost для построения дерева. Вот график первого дерева. Как вы видите из дерева, листья разделяются при одном и том же условии, например, 297, значение > 0.5.  
  

```
cat.plot_tree(tree_idx=0)
```

  
![](https://habrastorage.org/r/w1560/webt/f0/-g/_t/f0-g_tuqxwm1oacpqmdki-5xv5o.png)  
  
CatBoost дает нам словарь со всеми параметрами модели. Мы можем вывести их, как словарь.  
  

```
for key,value in cat.get_all_params().items(): print(‘{}, {}’.format(key,value))
```

  
![](https://habrastorage.org/r/w1560/webt/wg/u7/ei/wgu7eidujrpmdbyxfsgjkqgz244.png)  
  
В этой статье мы рассмотрели преимущества и ограничения CatBoost, а также ее основные параметры обучения. Затем мы реализовали простую регрессию с помощью scikit-learn. Надеюсь, вы получили достаточно информации об этой библиотеке, чтобы самостоятельно продолжить ее исследование.